# PR-04 — Web Article Ingestion Worker (Celery)

**Slice 2: Web Articles + Highlights**

This PR implements **asynchronous web article ingestion** using a Celery worker and a Node.js subprocess (Playwright + jsdom + @mozilla/readability).

It completes URL-based ingestion end-to-end:
- fetch → extract → sanitize → canonicalize → persist fragment
- resolves canonical URLs and performs deduplication
- transitions media to `ready_for_reading` or `failed`

This PR **must not block API requests** and **must not run ingestion in the API process**.

---

## 1. Goal

- `/media/from_url` enqueues ingestion and returns immediately
- A background worker:
  - fetches and renders the page (JS enabled)
  - extracts readable content
  - sanitizes HTML
  - generates canonical text
  - persists a single fragment (`idx = 0`)
  - resolves deduplication by canonical URL
  - updates processing state deterministically

---

## 2. Non-Goals

- No image proxy implementation (images may 404 until PR-05)
- No highlight logic
- No automatic Celery retries (manual retry via existing endpoint)
- No worker autoscaling
- No live-internet ingestion in CI gating tests

---

## 3. Architecture Overview

### Request Path (API)

```
POST /media/from_url
→ create provisional media row (canonical_url = NULL)
→ attach to viewer's default library
→ enqueue Celery task with (media_id, actor_user_id, request_id)
→ return 202 Accepted with ingest_enqueued=true
```

**PR-04 updates PR-03 behavior**: `/media/from_url` now returns `202 Accepted` (not `201 Created`) and sets `ingest_enqueued=true`. PR-03 tests must be updated accordingly.

### API Response (Updated)

```json
{
  "data": {
    "media_id": "<uuid>",
    "duplicate": false,
    "processing_status": "pending",
    "ingest_enqueued": true
  }
}
```

- `duplicate`: Always `false` at creation time (canonical URL unknown until fetch completes)
- `ingest_enqueued`: Always `true` in PR-04
- Clients discover deduplication via `GET /media/{id}` after ingestion completes

### Service Boundary

Route is transport-only; all logic lives in the service:

**Route** (`routes/media.py`):
```python
@router.post("/media/from_url", status_code=202)
def create_from_url(body: FromUrlRequest, viewer: Viewer = Depends(get_viewer)):
    result = services.media.enqueue_web_article_from_url(db, viewer, body.url, request_id)
    return success_response(result.model_dump(mode="json"))
```

**Service** (`services/media.py`):
```python
def enqueue_web_article_from_url(
    db: Session,
    viewer: Viewer,
    url: str,
    request_id: str | None,
) -> FromUrlResponse:
    # 1. Validate URL
    # 2. Create provisional media row
    # 3. Attach to viewer's default library
    # 4. Enqueue Celery task
    # 5. Return response
```

### Ingestion Path (Worker)

```
Celery task ingest_web_article(media_id, actor_user_id, request_id)
→ increment processing_attempts
→ mark extracting, set processing_started_at
→ run node subprocess (playwright + jsdom + readability)
→ resolve canonical_url (atomic transaction)
→ dedup by (kind, canonical_url) — uses actor_user_id for library attach
→ sanitize HTML (with base_url for relative URL resolution)
→ canonicalize text
→ persist fragment idx=0
→ mark ready_for_reading, set processing_completed_at
→ clear last_error_* fields on success
```

---

## 4. Worker Runtime Model

### Worker Image

- New Docker image: `docker/Dockerfile.worker`
- Includes:
  - Python runtime + Nexus backend deps
  - Node.js (LTS)
  - Playwright + Chromium
  - jsdom
  - @mozilla/readability

The API container **must not** include Node or Chromium.

### Node Package Structure

```
node/ingest/
├── package.json
├── package-lock.json
└── ingest.mjs
```

**package.json**:
```json
{
  "name": "nexus-ingest",
  "type": "module",
  "dependencies": {
    "@mozilla/readability": "^0.5.0",
    "jsdom": "^24.0.0",
    "playwright": "^1.40.0"
  }
}
```

### Docker Build Steps

```dockerfile
# In Dockerfile.worker
COPY node/ingest /app/node/ingest
WORKDIR /app/node/ingest
RUN npm ci --omit=dev
RUN npx playwright install --with-deps chromium
WORKDIR /app
```

Without these steps, PR-04 will only work locally.

---

## 5. Celery Task Definition

### Location

- `python/nexus/tasks/ingest_web_article.py`

### Task Signature

```python
@celery.task(bind=True, max_retries=0)
def ingest_web_article(
    self,
    media_id: UUID,
    actor_user_id: UUID,
    request_id: str | None = None,
) -> None:
    ...
```

**Why `actor_user_id`**: The worker cannot determine which user triggered ingestion. This ID is required for the dedup step — when a duplicate is detected, the worker must attach the existing media to the actor's default library.

**Why `max_retries=0`**: No automatic Celery retries in v1. Retry is manual via the existing retry endpoint. See §10.

### Invocation

- API enqueues with `apply_async(args=[media_id, actor_user_id], kwargs={"request_id": request_id}, queue="ingest")`
- Tests invoke with `task.apply(args=[media_id, actor_user_id])`

### Task Registration

Tasks must be explicitly imported — no autodiscovery:

**`python/nexus/tasks/__init__.py`**:
```python
from nexus.tasks.ingest_web_article import ingest_web_article

__all__ = ["ingest_web_article"]
```

**`apps/worker/main.py`**:
```python
from nexus.celery import celery_app
from nexus.tasks import ingest_web_article  # explicit import registers task

# Worker startup command:
# celery -A apps.worker.main:celery_app worker -Q ingest --concurrency=1
```

### Queue Routing

Use a dedicated queue for ingestion to control concurrency:

- Queue name: `ingest`
- Enqueue with: `apply_async(..., queue="ingest")`
- Worker startup: `celery -A apps.worker.main:celery_app worker -Q ingest --concurrency=1`

This enforces strict concurrency without a distributed semaphore.

---

## 6. Dependency Usage (Worker)

| Dependency | Source                 | Worker Pattern                          |
|------------|------------------------|-----------------------------------------|
| Settings   | `get_settings()`       | Direct call at startup                  |
| Database   | `get_session_factory()`| Explicit session lifecycle (try/finally)|
| Storage    | `get_storage_client()` | Direct call                             |

The worker does not use FastAPI DI.

---

## 7. Processing State Transitions

| Step       | State               | Fields Updated                                                              |
|------------|---------------------|-----------------------------------------------------------------------------|
| Task start | `extracting`        | `processing_attempts += 1`, `processing_started_at = now()`                 |
| Success    | `ready_for_reading` | `processing_completed_at = now()`, clear `last_error_code`, `last_error_message` |
| Failure    | `failed`            | `failed_at = now()`, `failure_stage=extract`, `last_error_code`, `last_error_message` |

### Idempotency Guard

Check **before** incrementing `processing_attempts`:

If:
- `processing_status = ready_for_reading`
- AND fragment `idx = 0` exists

→ task exits immediately (success), no state changes.

### Field Semantics

- `processing_attempts`: Incremented only when actually attempting ingestion (after idempotency guard passes)
- `processing_started_at`: Always set when entering `extracting`
- `processing_completed_at`: Set on success only; stays `NULL` on failure
- `failed_at`: Set on failure only
- `last_error_*`: Cleared on success (prevents stale errors from previous attempts)

---

## 8. Node Subprocess (Fetch + Extract)

### Location

- `node/ingest/ingest.mjs`

### Responsibilities

- Fetch page using Playwright (JS enabled)
- Follow redirects
- Capture final URL
- Extract `document.documentElement.outerHTML` (full HTML)
- Build DOM via jsdom with URL context: `new JSDOM(html, { url: final_url })`
- Run Mozilla Readability
- Return extracted content with base URL for relative URL resolution

### Browser Configuration

```javascript
// Deterministic user-agent
await page.setExtraHTTPHeaders({
  'User-Agent': 'NexusBot/1.0 (+https://nexus.example.com/bot)'
});

// Block heavy resources for speed and stability
await page.route('**/*', (route) => {
  const type = route.request().resourceType();
  if (['media', 'font', 'image'].includes(type)) {
    route.abort();
  } else {
    route.continue();
  }
});

// Timeouts
await page.goto(url, { timeout: 30000, waitUntil: 'domcontentloaded' });
```

This reduces ingest time and flakiness significantly.

### Input (stdin JSON)

```json
{
  "url": "https://example.com/article",
  "timeout_ms": 30000
}
```

### Output (stdout JSON)

```json
{
  "final_url": "https://example.com/actual-article",
  "base_url": "https://example.com/actual-article",
  "title": "Article Title",
  "content_html": "<div>...</div>"
}
```

- `final_url`: The URL after all redirects
- `base_url`: Same as `final_url`; used by Python sanitizer to resolve relative URLs
- `content_html`: Readability's extracted HTML (may contain relative URLs)

### Failure Semantics

| Exit Code | Meaning            | Worker Handling                                |
|-----------|--------------------|------------------------------------------------|
| 0         | Success            | Continue to sanitization                       |
| 10        | Timeout            | `E_INGEST_TIMEOUT`, `failure_stage=extract`    |
| 11        | Fetch failed       | `E_INGEST_FAILED`, `failure_stage=extract`     |
| 12        | Readability failed | `E_INGEST_FAILED`, `failure_stage=extract`     |

All failures use `E_INGEST_FAILED` (or `E_INGEST_TIMEOUT`) with `last_error_message` containing details.

**Note**: The DB enum for `failure_stage` is `upload | extract | transcribe | embed | other`. All web article ingestion failures use `extract`.

---

## 9. Timeouts + Isolation

### Python Subprocess Pattern

```python
proc = subprocess.Popen(
    ["node", "node/ingest/ingest.mjs"],
    stdin=subprocess.PIPE,
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    start_new_session=True,  # creates new process group
)

try:
    stdout, stderr = proc.communicate(input=json_input, timeout=40)
except subprocess.TimeoutExpired:
    os.killpg(proc.pid, signal.SIGKILL)  # kill entire process group
    proc.wait()
    # mark media as failed with E_INGEST_TIMEOUT
```

### Timeout Policy

- Hard wall-clock limit: 40 seconds
- On timeout: kill entire process group, mark media `failed` with `E_INGEST_TIMEOUT`
- Capture stdout/stderr with size limit (e.g. 1MB) to prevent memory issues from noisy pages

### Memory Isolation

Memory isolation is best-effort via container limits. No per-task memory enforcement in v1.

---

## 10. Concurrency & Retry Policy

### Concurrency

Concurrency is controlled by worker pool size and queue routing:
- `celery worker --concurrency=N` sets pool size
- Recommendation: start with N=1–2 due to Chromium memory cost
- No distributed semaphore in v1

### Retry Policy (v1: Manual Only)

No automatic Celery retries in v1:
- `max_retries=0` on the task
- On failure, media is marked `failed` with `last_error_*` populated
- User retries via existing retry endpoint, which schedules a new task

**Rationale**: Auto-retries without observability cause confusion. Add automatic retries once monitoring is in place.

### Error Codes

Keep error codes minimal; use `last_error_message` for debugging details:

| Error Code              | When Used                                      | `failure_stage` |
|-------------------------|------------------------------------------------|-----------------|
| `E_INGEST_TIMEOUT`      | Node subprocess timed out                      | `extract`       |
| `E_INGEST_FAILED`       | Fetch failed, readability failed, other errors | `extract`       |
| `E_SANITIZATION_FAILED` | Sanitization or canonicalization failed        | `extract`       |

**Note**: The DB enum for `failure_stage` is `upload | extract | transcribe | embed | other`. All web article ingestion failures use `extract` since the logical stage is content extraction.

---

## 11. Canonical URL Resolution + Dedup

### Canonical URL

Computed from `final_url`:
- lowercase scheme + host
- fragment stripped
- query preserved

### Dedup Algorithm (Atomic)

The dedup must be atomic to avoid race conditions:

```
1. Worker fetches page → gets final_url
2. Compute canonical_url from final_url
3. Open DB transaction:
   a. SELECT ... FOR UPDATE media row by id (ensure still exists)
   b. If already ready_for_reading AND fragment idx=0 exists → commit, exit success
   c. Try: UPDATE media SET canonical_url = :canon WHERE id = :id
   d. Flush (triggers unique constraint check)
4. If commit succeeds → continue to sanitization
5. If IntegrityError (duplicate canonical_url):
   a. Rollback
   b. Query winner: SELECT id FROM media WHERE kind = 'web_article' AND canonical_url = :canon
   c. Look up actor's default library:
      SELECT id FROM libraries WHERE owner_user_id = :actor_user_id AND is_default = true
   d. Single transaction (attach THEN delete to guarantee actor doesn't lose access):
      - INSERT INTO library_media (library_id, media_id) VALUES (:actor_default_lib, :winner_id)
        ON CONFLICT DO NOTHING
      - DELETE FROM media WHERE id = :loser_id
   e. Exit success (dedup complete)
```

**Why `actor_user_id` is required**: Step 5c-d attaches the winner media to the actor's default library. Without `actor_user_id`, the worker cannot determine which library to use.

**Critical ordering**: Attach winner to actor's library *before* deleting loser. This guarantees the actor never loses access to the content.

**Safety**: If the loser row was already removed from libraries, the delete is still safe (just deletes the orphan row).

---

## 12. Sanitization (Python)

### Location

- `python/nexus/services/sanitize_html.py`

### Function Signature

```python
def sanitize_html(html: str, base_url: str) -> str:
    ...
```

**Required input**: `base_url` (from node subprocess output) for resolving relative URLs.

### Processing Order

1. **Parse HTML into DOM**: Use `lxml.html` or similar to parse into a tree
2. **Resolve relative URLs**: For each `href` and `src` attribute, use `urllib.parse.urljoin(base_url, value)` to make absolute
3. **Apply Bleach allowlist**: Per L2 spec
4. **Strip attributes**: Remove `style`, `class`, `id`, all `on*`
5. **Disallow elements**: Remove `script`, `iframe`, `svg`, `form`, etc.
6. **Rewrite images**: Convert `<img src="...">` to `/media/image?url=<encoded_absolute_url>` (may 404 until PR-05)
7. **Rewrite links**: Add to `<a>`:
   - `rel="noopener noreferrer"`
   - `target="_blank"`
   - `referrerpolicy="no-referrer"`

**Critical**: Must use DOM parsing for URL resolution — string replace will be buggy. Resolve relative URLs (step 2) before image proxy rewriting (step 6).

Raw HTML is not persisted.

---

## 13. Canonical Text Generation

### Location

- `python/nexus/services/canonicalize.py`

### Output

- One fragment:
  - `idx = 0`
  - `html_sanitized`
  - `canonical_text`

Rules exactly match L2 spec (block boundaries, whitespace normalization, exclusions).

Fragments are immutable after persistence.

---

## 14. Capabilities Update

Update `derive_capabilities()`:
- For `web_article` + `ready_for_reading`:
  - `can_read = true`
  - `can_highlight = true`
  - `can_quote = true`
- Use `has_fragments=True` to gate highlight/quote capability.

---

## 15. Test Strategy

### HTTP Fixture Server

Use `pytest-httpserver` for deterministic HTTP fixtures:

```python
# conftest.py
import pytest
from pytest_httpserver import HTTPServer

@pytest.fixture
def httpserver_listen_address():
    return ("127.0.0.1", 0)  # random available port

@pytest.fixture
def fixture_server(httpserver: HTTPServer):
    return httpserver
```

**Alternative**: Run a tiny FastAPI app in-thread with uvicorn on a random port.

### Backend Integration Tests (pytest)

- Tests:
  - Redirect resolution (fixture server returns 301/302)
  - Dedup across redirects (two URLs → same canonical)
  - XSS sanitization (fixture serves malicious HTML)
  - `canonical_text` correctness
  - Fragment persistence
  - State transitions (`pending` → `extracting` → `ready_for_reading`)
  - Failure states (`failed` with correct `failure_stage`, `last_error_*`)

### Task Execution in Tests

- Call `ingest_web_article.apply(args=[media_id, actor_user_id])` directly — runs inline, no worker process
- No sleeps, no async waiting

### Localhost URL Allowlist

- In `NEXUS_ENV=test`, allow `127.0.0.1` and `localhost` URLs in the URL validator
- The node subprocess must also be able to fetch from localhost in test env
- Production remains strict (no localhost/loopback)

### Explicit Non-Goal

- Live-internet ingestion is not part of CI gating
- Can be added later as non-blocking canary

---

## 16. Deliverables

### New Files

| File | Purpose |
|------|---------|
| `docker/Dockerfile.worker` | Worker image with Python + Node + Playwright + Chromium |
| `docker-compose.worker.yml` | Dev/test worker service definition |
| `node/ingest/package.json` | Node dependencies for ingest script |
| `node/ingest/ingest.mjs` | Playwright + jsdom + Readability extraction |
| `python/nexus/tasks/__init__.py` | Task module exports |
| `python/nexus/tasks/ingest_web_article.py` | Celery task definition |
| `python/nexus/services/node_ingest.py` | Python wrapper for node subprocess protocol |
| `python/nexus/services/sanitize_html.py` | HTML sanitization with URL resolution |
| `apps/worker/main.py` | Worker entrypoint with task registration |

### Modified Files

| File | Change |
|------|--------|
| `routes/media.py` | Update `/media/from_url` to return 202, call enqueue service |
| `services/media.py` | Add `enqueue_web_article_from_url()` |
| `services/url_normalize.py` | Add localhost allowlist for `NEXUS_ENV=test` |

### Build Pipeline Note

Worker image requires Node.js dev dependencies at build time (for `npx playwright install`). Ensure CI builds the worker image separately from the API image.

---

## 17. Done Definition

PR-04 is complete when:
- `/media/from_url` returns 202 and enqueues ingestion
- Worker produces a readable web article end-to-end
- Deduplication works via canonical URL
- Sanitization and canonicalization are correct and immutable
- Processing state machine is deterministic
- All tests pass without network flakiness
- Worker image builds and runs in docker-compose
