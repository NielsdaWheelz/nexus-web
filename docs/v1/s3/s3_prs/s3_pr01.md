# PR-01 Spec — Slice 3 Schema Migration + Core Helpers + Web Ingest Hook

## goal
land the slice-3 schema foundation and the minimal core helpers that later PRs depend on, without adding any new public API routes or UI.

this PR is allowed to touch:
- alembic migration(s)
- backend db models + services helpers
- web ingestion pipeline hook to persist `fragment_block` for new ingests
- migration tests + helper unit tests

this PR must **not** add:
- chat routes (PR-02+)
- llm adapters (PR-04)
- send-message endpoint (PR-05)
- frontend code

---

## scope

### in scope
1) alembic migration `0004_slice3_schema.py`:
   - create new slice-3 tables
   - add generated stored `tsvector` columns + GIN indexes
   - add constraints and indexes required by S3 spec
2) backend core helpers:
   - conversation seq assignment helper (db-level locking pattern)
   - crypto helper for encrypt/decrypt user api keys (PyNaCl secretbox xchacha20-poly1305)
   - context-window helper using `fragment_block` when present, fallback ±600 chars
   - `fragment_block` parser from `canonical_text` (based on `\n\n`)
3) ingestion hook:
   - update `tasks/ingest_web_article.py` chain to create `fragment_block` rows for new fragments (idx=0) at ingest time
4) tests:
   - migration tests (constraints + generated columns + gin indexes)
   - concurrency test for `next_seq` locking
   - crypto roundtrip test
   - fragment_block parsing + context-window selection tests
   - ingest hook test: new ingest creates fragment_blocks

### out of scope
- any new HTTP routes
- any llm provider calls
- any byok UI flows
- any backfill for existing fragments (explicitly not required)
- any changes to canonicalization algorithm itself (only parsing existing canonical_text output)

---

## dependencies
- existing tables from slices 0–2: users, libraries, membership, media, fragment, highlight, annotation, library_media
- existing canonical_text generation rules and invariants (codepoint offsets, `\n\n` block separators)
- existing migration test harness at `python/tests/test_migrations.py`

---

## deliverables

## 1) alembic migration: `migrations/alembic/versions/0004_slice3_schema.py`

### 1.1 create tables

#### `conversation`
columns:
- id UUID PK default gen_random_uuid()
- owner_user_id UUID not null fk → users(id) on delete cascade
- sharing TEXT not null default 'private' check in ('private','library','public')
- next_seq INTEGER not null default 1 check(next_seq >= 1)
- created_at timestamptz not null default now()
- updated_at timestamptz not null default now()

indexes/constraints:
- idx_conversation_owner_updated_at on (owner_user_id, updated_at desc)

#### `conversation_share`
columns:
- conversation_id UUID not null fk → conversation(id) on delete cascade
- library_id UUID not null fk → library(id) on delete cascade
- created_at timestamptz not null default now()

constraints:
- pk (conversation_id, library_id)

#### `models`
columns:
- id UUID PK default gen_random_uuid()
- provider TEXT not null check in ('openai','anthropic','gemini')
- model_name TEXT not null
- max_context_tokens INTEGER not null check(max_context_tokens > 0)
- cost_per_1k_input_tokens_usd INTEGER null  -- micros
- cost_per_1k_output_tokens_usd INTEGER null -- micros
- is_available BOOLEAN not null default true

constraints:
- unique(provider, model_name)

#### `message`
columns:
- id UUID PK default gen_random_uuid()
- conversation_id UUID not null fk → conversation(id) on delete cascade
- seq INTEGER not null check(seq >= 1)
- role TEXT not null check in ('user','assistant','system')
- content TEXT not null
- status TEXT not null default 'complete' check in ('pending','complete','error')
- error_code TEXT null
- model_id UUID null fk → models(id) on delete set null
- created_at timestamptz not null default now()
- updated_at timestamptz not null default now()

constraints:
- unique(conversation_id, seq)  name: uix_messages_conversation_seq
- check: (status != 'pending' OR role = 'assistant') name: ck_message_pending_only_assistant

indexes:
- idx_message_conversation_seq on (conversation_id, seq)

#### `message_llm`
1:1 with assistant message.
columns:
- message_id UUID pk fk → message(id) on delete cascade
- provider TEXT not null check in ('openai','anthropic','gemini')
- model_name TEXT not null
- prompt_tokens INTEGER null check(prompt_tokens >= 0)
- completion_tokens INTEGER null check(completion_tokens >= 0)
- total_tokens INTEGER null check(total_tokens >= 0)
- key_mode_requested TEXT not null check in ('auto','byok_only','platform_only')
- key_mode_used TEXT not null check in ('platform','byok')
- cost_usd_micros INTEGER null check(cost_usd_micros >= 0)
- latency_ms INTEGER null check(latency_ms >= 0)
- error_class TEXT null
- prompt_version TEXT not null
- created_at timestamptz not null default now()

notes:
- `key_mode_requested`: what the client asked for (auto|byok_only|platform_only)
- `key_mode_used`: what was actually used after resolution (platform|byok)
- this allows audit trails to explain fallback behavior (e.g., auto → platform when no BYOK key)

#### `user_api_key`
columns:
- id UUID pk default gen_random_uuid()
- user_id UUID not null fk → users(id) on delete cascade
- provider TEXT not null check in ('openai','anthropic','gemini')
- encrypted_key BYTEA not null
- key_nonce BYTEA not null
- master_key_version INTEGER not null default 1 check(master_key_version > 0)
- key_fingerprint TEXT not null  -- last4 or stable short fingerprint; must not be empty
- status TEXT not null default 'untested' check in ('untested','valid','invalid','revoked')
- created_at timestamptz not null default now()
- last_tested_at timestamptz null
- revoked_at timestamptz null

constraints:
- unique(user_id, provider) name: uix_user_api_key_user_provider
- check: octet_length(key_nonce)=24 name: ck_user_api_key_nonce_len

#### `idempotency_keys`
columns:
- user_id UUID not null fk → users(id) on delete cascade
- key TEXT not null
- payload_hash TEXT not null
- user_message_id UUID not null fk → message(id) on delete cascade
- assistant_message_id UUID not null fk → message(id) on delete cascade
- created_at timestamptz not null default now()
- expires_at timestamptz not null

constraints:
- pk (user_id, key)
- check: length(key) >= 1 and length(key) <= 128 name: ck_idempotency_key_length

indexes:
- idx_idempotency_user_created on (user_id, created_at desc)
- idx_idempotency_expires_at on (expires_at)

notes:
- composite PK (user_id, key) prevents cross-user collisions and accidental leaks
- key length bounded to 128 chars to match BFF header normalization
- service sets `expires_at = now() + interval '24 hours'` at insert time

#### `message_context`
columns:
- id UUID pk default gen_random_uuid()
- message_id UUID not null fk → message(id) on delete cascade
- target_type TEXT not null check in ('media','highlight','annotation')
- ordinal INTEGER not null check(ordinal >= 0)
- media_id UUID null fk → media(id) on delete cascade
- highlight_id UUID null fk → highlight(id) on delete cascade
- annotation_id UUID null fk → annotation(id) on delete cascade
- created_at timestamptz not null default now()

constraints:
- unique(message_id, ordinal) name: uix_message_context_message_ordinal
- check exactly one target fk present name: ck_message_context_one_target
  - implemented as:
    ( (media_id is not null)::int
    + (highlight_id is not null)::int
    + (annotation_id is not null)::int ) = 1

indexes:
- idx_message_context_message on (message_id)

notes:
- v1 supports only media/highlight/annotation as context targets
- message and conversation context types deferred to later slice (avoids cascade complexity where deleting a referenced message would delete contexts in other messages, violating the "exactly one target" constraint after SET NULL)

#### `conversation_media`
columns:
- conversation_id UUID not null fk → conversation(id) on delete cascade
- media_id UUID not null fk → media(id) on delete cascade
- last_message_at timestamptz not null default now()

constraints:
- pk (conversation_id, media_id)

indexes:
- idx_conversation_media_media on (media_id)

#### `fragment_block`
columns:
- id UUID pk default gen_random_uuid()
- fragment_id UUID not null fk → fragment(id) on delete cascade
- block_idx INTEGER not null check(block_idx >= 0)
- start_offset INTEGER not null check(start_offset >= 0)
- end_offset INTEGER not null check(end_offset >= start_offset)
- block_type TEXT null
- is_empty BOOLEAN not null default false

constraints:
- unique(fragment_id, block_idx) name: uix_fragment_block_fragment_idx
- check: start_offset <= end_offset name: ck_fragment_block_offsets

indexes:
- idx_fragment_block_fragment_offsets on (fragment_id, start_offset, end_offset)

notes:
- block_type is nullable and is not populated in v1.
- is_empty supports "contiguous coverage" while allowing context algorithm to skip empty blocks.
- **delimiter handling**: the `\n\n` separator is included at the END of the preceding block's `[start, end)` range, so coverage is contiguous (block[n].end == block[n+1].start). the final block ends at len(canonical_text) with no trailing delimiter.

### 1.2 add generated tsvector columns + gin indexes

add columns:
- media.title_tsv tsvector generated always as (to_tsvector('english', coalesce(title,''))) stored
- fragment.canonical_text_tsv tsvector generated always as (to_tsvector('english', coalesce(canonical_text,''))) stored
- annotation.body_tsv tsvector generated always as (to_tsvector('english', coalesce(body,''))) stored
- message.content_tsv tsvector generated always as (to_tsvector('english', coalesce(content,''))) stored

add GIN indexes:
- idx_media_title_tsv gin(title_tsv)
- idx_fragment_canonical_text_tsv gin(canonical_text_tsv)
- idx_annotation_body_tsv gin(body_tsv)
- idx_message_content_tsv gin(content_tsv)

migration comment:
- in prod, large tables may require concurrent index creation; v1 deploy runbook can handle that outside this PR.

---

## 2) backend code changes

### 2.1 db models: `python/nexus/db/models.py`
- add ORM models for all new tables above
- ensure enums are implemented as constrained strings matching DB checks (do not introduce python-only enums without db checks)
- update existing models for new generated columns (optional; they can be excluded from ORM if not needed yet, but must not break)

### 2.2 seq helper: `python/nexus/services/seq.py`
add:
- `assign_next_message_seq(db: Session, conversation_id: UUID) -> int`
  - executes:
    - SELECT ... FOR UPDATE conversation row
    - read next_seq
    - UPDATE next_seq = next_seq + 1
    - return original next_seq

constraints:
- must be called inside an existing transaction context
- must not open/commit its own transaction

### 2.3 crypto helper: `python/nexus/services/crypto.py`
add:
- `encrypt_secretbox(plaintext: bytes, nonce: bytes) -> bytes`
- `decrypt_secretbox(ciphertext: bytes, nonce: bytes) -> bytes`
- `require_master_key() -> bytes` loads from env `NEXUS_KEY_ENCRYPTION_KEY` (base64) and validates length=32

implementation requirements:
- use PyNaCl SecretBox with XChaCha20-Poly1305 (libsodium)
- nonce length must be 24 bytes; enforced both in code and by db check
- never log plaintext or ciphertext
- include unit tests

config requirements:
- add `NEXUS_KEY_ENCRYPTION_KEY` to `python/nexus/config.py` with validation for test/dev
- docker-compose/test env must include a deterministic key for tests

### 2.4 fragment_block parsing: `python/nexus/services/fragment_blocks.py`
add:
- `parse_fragment_blocks(canonical_text: str) -> list[FragmentBlockSpec]`
  - scans canonical_text and produces contiguous blocks using `\n\n` separators
  - **delimiter ownership**: `\n\n` belongs to the END of the preceding block's range, so:
    - block[n].end includes the `\n\n` that follows it (if any)
    - block[n+1].start == block[n].end (contiguous, no gaps)
    - final block ends at len(canonical_text) with no trailing delimiter
  - includes empty blocks to preserve contiguity; sets is_empty = (block_text.strip() == '')
  - block offsets are codepoint indices (python str indexing is codepoint)
  - returns specs with:
    - block_idx (0..n-1)
    - start_offset, end_offset (half-open)
    - is_empty
  - **invariant**: block[0].start == 0, block[-1].end == len(canonical_text)
- `insert_fragment_blocks(db: Session, fragment_id: UUID, blocks: list[FragmentBlockSpec]) -> None`
  - bulk insert rows

### 2.5 context window helper: `python/nexus/services/context_window.py`
add:
- `get_context_window(db: Session, fragment_id: UUID, start_offset: int, end_offset: int) -> ContextWindow`
  - tries block-based window if any `fragment_block` exists for fragment
  - **invariant**: returned window ALWAYS fully contains `[start_offset, end_offset)` slice
  - selection (block-based):
    - find containing block by start_offset
    - choose prev and next non-empty blocks (skip is_empty)
    - include containing block always
    - compute raw window bounds: `window_start`, `window_end`
    - **clamp**: ensure `window_start <= start_offset` and `window_end >= end_offset`
    - apply 2,500 char cap by shrinking from edges, never cutting into `[start_offset, end_offset)`:
      - if raw window > 2500: trim from start (up to start_offset) then from end (down to end_offset)
    - concatenate block slices with `\n\n` between blocks
  - fallback (no blocks):
    - pull fragment.canonical_text
    - compute `window_start = max(0, start_offset - 600)`
    - compute `window_end = min(len(text), end_offset + 600)`
    - **clamp**: ensure `window_start <= start_offset` and `window_end >= end_offset`
    - apply 2,500 cap same way (shrink edges, preserve selection)
  - returns:
    - `text: str`
    - `source: Literal['blocks','fallback']`

### 2.6 ingestion hook: `python/nexus/tasks/ingest_web_article.py` (and any called service)
- after canonical_text is produced and before/with fragment insert:
  - parse blocks from canonical_text
  - insert fragment and fragment_blocks in same transaction
- if fragment insert fails, blocks must not persist (transactional integrity)

notes:
- no backfill; only new ingests get blocks.

---

## 3) tests

### 3.1 migration tests: extend `python/tests/test_migrations.py`
add tests for:
- `message` constraint: pending only assistant (ck_message_pending_only_assistant)
- `message` unique (conversation_id, seq) (uix_messages_conversation_seq)
- `user_api_key` unique (user_id, provider) and nonce length check
- `message_context` one-target check (now only 3 target types: media/highlight/annotation)
- `idempotency_keys` composite pk (user_id, key) and key length check (ck_idempotency_key_length)
- `message_llm` key_mode_requested and key_mode_used enum checks
- generated columns exist and are STORED:
  - verify `information_schema.columns.is_generated = 'ALWAYS'` for each new tsv column
- gin indexes exist:
  - query `pg_indexes` and assert indexdef contains `USING gin` for each index

pattern:
- create minimal rows via raw SQL inserts; then attempt violating inserts; assert constraint/index name appears in IntegrityError message.

### 3.2 seq concurrency test: new file `python/tests/test_seq_locking.py`
- create conversation row
- session A:
  - begin tx
  - select conversation for update
  - call assign_next_message_seq (or direct SQL equivalent)
  - sleep/hold
- session B (separate thread/session):
  - attempt assign_next_message_seq must block until A commits
- assert returned seq values are consecutive (1 then 2)
- keep timeouts small to avoid flakiness; fail test if B does not block

### 3.3 crypto tests: new file `python/tests/test_crypto.py`
- encrypt/decrypt roundtrip returns original plaintext
- different nonce yields different ciphertext
- wrong nonce or wrong master key fails decryption (raises)

### 3.4 fragment_block + context window tests: new file `python/tests/test_fragment_blocks.py`
- parse canonical_text with known `\n\n` separators and verify:
  - contiguous coverage: block0.start=0, last.end=len(text)
  - delimiter ownership: `\n\n` included at end of preceding block
  - empty blocks present and flagged
- insert blocks and verify db rows count matches
- context window:
  - when blocks exist: returns source='blocks' and includes adjacent blocks
  - when blocks missing: returns source='fallback'
  - **selection preservation**: window always fully contains `[start_offset, end_offset)` even after cap
  - cap enforcement: if window > 2500, shrinks edges without cutting into selection

### 3.5 ingest hook test: extend existing ingest web article tests
- after ingest completes for a new article:
  - fragment exists (idx=0)
  - fragment_block rows exist for that fragment
  - block_idx unique and ordered
  - coverage invariant holds

---

## constraints / non-goals
- no new routes
- no modifications to existing highlight behavior
- no attempt to classify block types
- no backfill of fragment_blocks for existing media
- no provider libs added here besides crypto dependency

---

## completion checklist
- [ ] migration 0004 upgrades cleanly on fresh db
- [ ] migration tests pass and assert new constraints + generated columns + gin indexes
- [ ] existing tests pass
- [ ] `NEXUS_KEY_ENCRYPTION_KEY` required in test env and present in docker-compose/test config
- [ ] web article ingest creates fragment_block rows transactionally
- [ ] helpers have unit tests (seq, crypto, blocks/context)
- [ ] no new api routes or frontend changes