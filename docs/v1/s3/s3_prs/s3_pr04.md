# PR-04 Spec — LLM Adapter Layer (OpenAI, Anthropic, Gemini)

This PR introduces the provider-agnostic LLM adapter layer used by Slice 3.
It implements **all three providers**, **non-streaming + streaming**, **error normalization**, and **prompt rendering**, but does **not** expose any public routes.

This PR is pure backend infrastructure and is a hard dependency for PR-05.

---

## 0) Scope and Non-Scope

### In Scope

- Provider adapters for:
  - OpenAI
  - Anthropic
  - Gemini
- Unified **async** adapter interface
- Prompt rendering (provider-agnostic)
- Error classification and normalization
- Streaming support (streaming-friendly chunks, no SSE formatting)
- Feature-flag enforcement
- Provider availability gating
- Deterministic tests with mocked HTTP

### Explicitly Out of Scope

- Send-message endpoint (PR-05)
- SSE framing (PR-05/PR-08)
- Idempotency
- Rate limiting
- Token budgets
- Persistence of messages
- Frontend changes

---

## 1) Directory Structure

```
python/nexus/services/llm/
├── __init__.py
├── adapter.py          # Abstract base interface
├── router.py           # Adapter selection + error normalization
├── prompt.py           # Prompt + context rendering (provider-agnostic)
├── types.py            # Shared dataclasses
├── errors.py           # Error classification logic
├── openai_adapter.py
├── anthropic_adapter.py
└── gemini_adapter.py

python/tests/fixtures/llm/
├── openai/
│   ├── chat_completion_success.json
│   ├── chat_completion_stream_chunks.txt
│   └── error_*.json
├── anthropic/
│   └── ...
└── gemini/
    └── ...
```

---

## 2) Core Types (`types.py`)

```python
from dataclasses import dataclass
from typing import Literal, AsyncIterator

@dataclass
class Turn:
    """Provider-agnostic conversation turn."""
    role: Literal["system", "user", "assistant"]
    content: str

@dataclass
class LLMRequest:
    """Request to LLM adapter."""
    model_name: str
    messages: list[Turn]  # system turn first if present
    max_tokens: int
    temperature: float | None = None

@dataclass
class LLMUsage:
    """Token usage from provider response."""
    prompt_tokens: int | None
    completion_tokens: int | None
    total_tokens: int | None

@dataclass
class LLMResponse:
    """Complete response from non-streaming call."""
    text: str
    usage: LLMUsage | None
    provider_request_id: str | None
```

### Streaming Chunk

```python
@dataclass
class LLMChunk:
    """Single chunk from streaming response."""
    delta_text: str
    done: bool
    usage: LLMUsage | None = None
    provider_request_id: str | None = None
```

**Streaming invariants:**
- Chunks with `done=False` **must** have `usage=None`
- Exactly **one** terminal chunk with `done=True`
- Terminal chunk **may** have `usage` and `provider_request_id` (if provider returns them)
- If provider stream ends without terminal marker: raise `E_LLM_PROVIDER_DOWN`

---

## 3) Adapter Interface (`adapter.py`)

**Decision: Async adapters with `httpx.AsyncClient`**

Why: FastAPI is async. Blocking HTTP inside async endpoints stalls the event loop. Streaming with sync httpx inside FastAPI is error-prone.

```python
from abc import ABC, abstractmethod
from typing import AsyncIterator

class LLMAdapter(ABC):
    @abstractmethod
    async def generate(
        self,
        req: LLMRequest,
        *,
        api_key: str,
        timeout_s: int,
    ) -> LLMResponse:
        """Non-streaming generation. Returns complete response."""
        pass

    @abstractmethod
    async def generate_stream(
        self,
        req: LLMRequest,
        *,
        api_key: str,
        timeout_s: int,
    ) -> AsyncIterator[LLMChunk]:
        """Streaming generation. Yields chunks until done=True."""
        pass
```

### Rules

- No retries inside adapters
- No DB access
- No logging of request/response bodies
- Raw provider errors bubble up to router for classification
- Each adapter handles Turn → provider format conversion internally

---

## 4) Provider Adapters

### 4.1 OpenAI (`openai_adapter.py`)

**Endpoint:** `POST https://api.openai.com/v1/chat/completions`

**Headers:**
```
Authorization: Bearer <key>
Content-Type: application/json
```

**Request body (minimal required fields):**
```json
{
  "model": "<model_name>",
  "messages": [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "..."}
  ],
  "max_tokens": 1024,
  "temperature": 0.7,
  "stream": false
}
```

**Response (non-stream) — extract:**
```json
{
  "id": "chatcmpl-...",
  "choices": [{"message": {"content": "<output_text>"}}],
  "usage": {
    "prompt_tokens": 100,
    "completion_tokens": 50,
    "total_tokens": 150
  }
}
```
- `text` = `choices[0].message.content`
- `usage` = direct mapping
- `provider_request_id` = response header `x-request-id` or body `id`

**Streaming:**
- Set `"stream": true`
- Response: Server-Sent Events
- Each event: `data: {"choices":[{"delta":{"content":"..."}}]}`
- Terminal event: `data: [DONE]`
- Usage may appear in final chunk (OpenAI returns it in stream if requested)

---

### 4.2 Anthropic (`anthropic_adapter.py`)

**Endpoint:** `POST https://api.anthropic.com/v1/messages`

**Headers:**
```
x-api-key: <key>
anthropic-version: 2023-06-01
Content-Type: application/json
```

**Request body:**
```json
{
  "model": "<model_name>",
  "max_tokens": 1024,
  "temperature": 0.7,
  "system": "<system_prompt>",
  "messages": [
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "..."}
  ]
}
```

**Turn conversion:**
- System turn extracted to separate `"system"` field (Anthropic doesn't use system in messages array)
- Remaining turns mapped to `messages` with role preserved

**Response (non-stream):**
```json
{
  "id": "msg_...",
  "content": [{"type": "text", "text": "<output_text>"}],
  "usage": {
    "input_tokens": 100,
    "output_tokens": 50
  }
}
```
- `text` = concatenate all `content[].text` where `type="text"`
- `usage.prompt_tokens` = `input_tokens`
- `usage.completion_tokens` = `output_tokens`
- `usage.total_tokens` = sum
- `provider_request_id` = `id`

**Streaming:**
- Set `"stream": true`
- Response: Server-Sent Events
- Events: `event: content_block_delta` with `data: {"delta": {"text": "..."}}`
- Terminal: `event: message_stop`
- Usage in `event: message_delta` at end

---

### 4.3 Gemini (`gemini_adapter.py`)

**Non-streaming endpoint:** `POST https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent`

**Streaming endpoint:** `POST https://generativelanguage.googleapis.com/v1beta/models/{model}:streamGenerateContent?alt=sse`

**Auth:**
- Header: `x-goog-api-key: <key>`
- **Never** put key in query param (even though API supports it)
- **Never** log URL if key accidentally in query

**Request body:**
```json
{
  "contents": [
    {"role": "user", "parts": [{"text": "..."}]},
    {"role": "model", "parts": [{"text": "..."}]}
  ],
  "systemInstruction": {"parts": [{"text": "<system_prompt>"}]},
  "generationConfig": {
    "maxOutputTokens": 1024,
    "temperature": 0.7
  }
}
```

**Turn conversion:**
- System turn → `systemInstruction.parts[0].text`
- `"assistant"` role → `"model"` role in Gemini
- Each turn's content → `parts: [{"text": "..."}]`

**Response (non-stream):**
```json
{
  "candidates": [{
    "content": {"parts": [{"text": "<output_text>"}]}
  }],
  "usageMetadata": {
    "promptTokenCount": 100,
    "candidatesTokenCount": 50,
    "totalTokenCount": 150
  }
}
```
- `text` = concatenate `candidates[0].content.parts[].text`
- `usage.prompt_tokens` = `promptTokenCount`
- `usage.completion_tokens` = `candidatesTokenCount`
- `usage.total_tokens` = `totalTokenCount`
- `provider_request_id` = None (Gemini doesn't return one)

**Streaming:**
- Use `:streamGenerateContent?alt=sse` endpoint
- Response: Server-Sent Events
- Each event: `data: {"candidates":[{"content":{"parts":[{"text":"..."}]}}]}`
- Terminal: last event has `"finishReason": "STOP"`
- Usage in final event's `usageMetadata`

---

## 5) Error Classification (`errors.py`)

### Classification Function

```python
from enum import Enum

class LLMErrorClass(str, Enum):
    INVALID_KEY = "E_LLM_INVALID_KEY"
    RATE_LIMIT = "E_LLM_RATE_LIMIT"
    CONTEXT_TOO_LARGE = "E_LLM_CONTEXT_TOO_LARGE"
    TIMEOUT = "E_LLM_TIMEOUT"
    PROVIDER_DOWN = "E_LLM_PROVIDER_DOWN"
    MODEL_NOT_AVAILABLE = "E_MODEL_NOT_AVAILABLE"

def classify_provider_error(
    provider: str,
    status_code: int | None,
    json_body: dict | None,
    exception: Exception | None,
) -> LLMErrorClass:
    """
    Classify provider error into normalized error class.
    Called by router after catching adapter exceptions.
    """
```

### Classification Rules

**OpenAI:**
| Condition | Error Class |
|-----------|-------------|
| status 401 or 403 | `INVALID_KEY` |
| status 429 | `RATE_LIMIT` |
| status 400 + `error.code == "context_length_exceeded"` | `CONTEXT_TOO_LARGE` |
| status 400 + `"maximum context length"` in message | `CONTEXT_TOO_LARGE` |
| `httpx.TimeoutException` | `TIMEOUT` |
| status 5xx or `httpx.NetworkError` | `PROVIDER_DOWN` |
| status 404 + model not found | `MODEL_NOT_AVAILABLE` |

**Anthropic:**
| Condition | Error Class |
|-----------|-------------|
| status 401 or 403 | `INVALID_KEY` |
| status 429 | `RATE_LIMIT` |
| status 400 + `error.type == "invalid_request_error"` + `"too long"` in message | `CONTEXT_TOO_LARGE` |
| `httpx.TimeoutException` | `TIMEOUT` |
| status 5xx or network error | `PROVIDER_DOWN` |
| status 404 | `MODEL_NOT_AVAILABLE` |

**Gemini:**
| Condition | Error Class |
|-----------|-------------|
| status 401 or 403 or `"API_KEY_INVALID"` in body | `INVALID_KEY` |
| status 429 or `"RESOURCE_EXHAUSTED"` | `RATE_LIMIT` |
| `"exceeds the maximum"` in message | `CONTEXT_TOO_LARGE` |
| `httpx.TimeoutException` | `TIMEOUT` |
| status 5xx or network error | `PROVIDER_DOWN` |
| status 404 or `"model not found"` | `MODEL_NOT_AVAILABLE` |

**Safety refusals:** Returned as normal assistant text (not an error). Check `finish_reason` for `"content_filter"` but still return the (possibly empty) text.

---

## 6) Router (`router.py`)

### Adapter Resolution

```python
class LLMRouter:
    def __init__(self, client: httpx.AsyncClient, settings: Settings):
        self._client = client
        self._settings = settings
        self._adapters = {
            "openai": OpenAIAdapter(client),
            "anthropic": AnthropicAdapter(client),
            "gemini": GeminiAdapter(client),
        }

    def resolve_adapter(self, provider: str) -> LLMAdapter:
        """Get adapter for provider, checking feature flags."""
        if not self._is_provider_enabled(provider):
            raise LLMError(LLMErrorClass.MODEL_NOT_AVAILABLE, f"Provider {provider} is disabled")
        return self._adapters[provider]

    def _is_provider_enabled(self, provider: str) -> bool:
        flags = {
            "openai": self._settings.ENABLE_OPENAI,
            "anthropic": self._settings.ENABLE_ANTHROPIC,
            "gemini": self._settings.ENABLE_GEMINI,
        }
        return flags.get(provider, False)
```

### Wrapped Calls with Error Normalization

```python
async def generate(self, provider: str, req: LLMRequest, api_key: str) -> LLMResponse:
    adapter = self.resolve_adapter(provider)
    try:
        return await adapter.generate(req, api_key=api_key, timeout_s=45)
    except httpx.TimeoutException:
        raise LLMError(LLMErrorClass.TIMEOUT, "Request timed out")
    except httpx.HTTPStatusError as e:
        error_class = classify_provider_error(provider, e.response.status_code, e.response.json(), None)
        raise LLMError(error_class, str(e))
    except httpx.NetworkError:
        raise LLMError(LLMErrorClass.PROVIDER_DOWN, "Network error")

async def generate_stream(self, provider: str, req: LLMRequest, api_key: str) -> AsyncIterator[LLMChunk]:
    adapter = self.resolve_adapter(provider)
    try:
        async for chunk in adapter.generate_stream(req, api_key=api_key, timeout_s=45):
            yield chunk
    except httpx.TimeoutException:
        raise LLMError(LLMErrorClass.TIMEOUT, "Request timed out")
    # ... similar error handling
```

---

## 7) Prompt Rendering (`prompt.py`)

### Responsibility

**prompt.py is provider-agnostic.** It produces a list of `Turn` objects. Each adapter handles conversion to provider-specific format.

### System Prompt (v1, fixed)

```
You are a careful assistant.
Answer only using the provided context when possible.
Quote directly when citing.
If information is missing or uncertain, say so.
```

### Render Function

```python
def render_prompt(
    user_content: str,
    history: list[Turn],
    context_blocks: list[str],
    system_prompt: str = DEFAULT_SYSTEM_PROMPT,
) -> list[Turn]:
    """
    Build provider-agnostic turn list.

    Args:
        user_content: Current user message
        history: Previous turns (may include prior system if multi-turn)
        context_blocks: Pre-rendered context strings (from context_window helper)
        system_prompt: System instructions (uses default v1 if not specified)

    Returns:
        List of Turn objects ready for adapter consumption.
        System turn always first (if present).
    """
    turns = []

    # System prompt with context
    full_system = system_prompt
    if context_blocks:
        context_section = "\n\n---\nContext:\n" + "\n\n".join(context_blocks)
        full_system = system_prompt + context_section

    turns.append(Turn(role="system", content=full_system))

    # History (user/assistant only, skip any old system turns)
    for turn in history:
        if turn.role in ("user", "assistant"):
            turns.append(turn)

    # Current user message
    turns.append(Turn(role="user", content=user_content))

    return turns
```

### Validation

```python
def validate_prompt_size(turns: list[Turn], max_chars: int = 100_000) -> None:
    """Raise E_CONTEXT_TOO_LARGE if total chars exceed limit."""
    total = sum(len(t.content) for t in turns)
    if total > max_chars:
        raise PromptTooLargeError(f"Prompt size {total} exceeds max {max_chars}")
```

---

## 8) HTTP Client Lifecycle

### FastAPI Integration

```python
# python/nexus/app.py

from contextlib import asynccontextmanager
import httpx

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    app.state.httpx_client = httpx.AsyncClient(
        timeout=httpx.Timeout(connect=10.0, read=45.0, write=10.0),
        limits=httpx.Limits(max_connections=100, max_keepalive_connections=20),
    )
    app.state.llm_router = LLMRouter(app.state.httpx_client, get_settings())

    yield

    # Shutdown
    await app.state.httpx_client.aclose()

app = FastAPI(lifespan=lifespan)
```

### Dependency Injection

```python
# python/nexus/dependencies.py

def get_llm_router(request: Request) -> LLMRouter:
    return request.app.state.llm_router
```

### Why This Matters

- Keeps things testable (inject mock client in tests)
- Avoids hidden globals
- Proper connection pooling and cleanup

---

## 9) Feature Flags

### Location

```python
# python/nexus/config.py

class Settings(BaseSettings):
    # ... existing settings ...

    ENABLE_OPENAI: bool = True
    ENABLE_ANTHROPIC: bool = True
    ENABLE_GEMINI: bool = True
```

### Enforcement

- `LLMRouter.resolve_adapter()` checks flag before returning adapter
- If disabled: raises `LLMError(LLMErrorClass.MODEL_NOT_AVAILABLE, "Provider disabled")`
- Flags gate **provider**, not individual models (v1 simplicity)

### Test Override

```python
def test_disabled_provider(monkeypatch):
    monkeypatch.setattr(settings, "ENABLE_OPENAI", False)
    with pytest.raises(LLMError) as exc:
        router.resolve_adapter("openai")
    assert exc.value.error_class == LLMErrorClass.MODEL_NOT_AVAILABLE
```

---

## 10) Logging Rules

**Allowed:**
- `provider`
- `model_name`
- `latency_ms`
- `error_class`
- `provider_request_id` (if any)

**Forbidden:**
- API keys
- Request bodies
- Response bodies
- Prompt text
- User content

---

## 11) Tests

### Fixture Strategy

Store mock responses in `python/tests/fixtures/llm/{provider}/`:
- `success_nonstream.json`
- `success_stream_chunks.txt` (newline-delimited events)
- `error_401.json`
- `error_429.json`
- `error_context_too_large.json`
- `error_500.json`

### Minimum Coverage Per Provider

| Test | Description |
|------|-------------|
| `test_{provider}_nonstream_success` | Happy path, verify text + usage extraction |
| `test_{provider}_stream_success` | Multiple chunks + terminal, verify accumulation |
| `test_{provider}_stream_chunks_before_done` | Verify `usage=None` on non-terminal chunks |
| `test_{provider}_invalid_key_401` | 401 → `E_LLM_INVALID_KEY` |
| `test_{provider}_rate_limit_429` | 429 → `E_LLM_RATE_LIMIT` |
| `test_{provider}_context_too_large` | Context error → `E_LLM_CONTEXT_TOO_LARGE` |
| `test_{provider}_provider_down_500` | 5xx → `E_LLM_PROVIDER_DOWN` |
| `test_{provider}_timeout` | Timeout → `E_LLM_TIMEOUT` |

**Total: 8 tests × 3 providers = 24 adapter tests minimum**

### Additional Tests

- `test_router_disabled_provider` — feature flag enforcement
- `test_prompt_render_with_context` — context block injection
- `test_prompt_render_no_context` — basic render
- `test_prompt_size_validation` — reject oversized prompts
- `test_turn_conversion_anthropic_system` — system extracted to separate field
- `test_turn_conversion_gemini_role_mapping` — assistant → model

### Test Tools

```python
import respx
from httpx import Response

@pytest.fixture
def mock_openai():
    with respx.mock(base_url="https://api.openai.com") as respx_mock:
        yield respx_mock

def test_openai_nonstream_success(mock_openai, llm_router):
    fixture = load_fixture("llm/openai/success_nonstream.json")
    mock_openai.post("/v1/chat/completions").respond(200, json=fixture)

    req = LLMRequest(model_name="gpt-4", messages=[...], max_tokens=100)
    response = await llm_router.generate("openai", req, api_key="sk-test")

    assert response.text == "Expected output"
    assert response.usage.total_tokens == 150
```

### Explicitly Forbidden

- Live provider calls in CI
- Real API keys anywhere in test code
- Flaky tests depending on network

---

## 12) Acceptance Criteria

- [ ] All three providers implemented (async)
- [ ] Streaming + non-streaming supported
- [ ] Error classification centralized and deterministic
- [ ] Feature flags enforced at provider level
- [ ] No secrets logged
- [ ] Shared `httpx.AsyncClient` via app.state
- [ ] Turn conversion handles Anthropic system field and Gemini role mapping
- [ ] Streaming invariants enforced (exactly one terminal chunk)
- [ ] All 24+ adapter tests pass without network access
- [ ] Prompt rendering is provider-agnostic

---

## 13) Risks & Mitigations

| Risk | Mitigation |
|------|------------|
| Provider API drift | Pin API versions in headers; central fixtures |
| Silent token overuse | Usage stored when available; null allowed |
| Streaming bugs | Comprehensive chunk tests with mocked payloads |
| Key leakage | Strict logging rules + adapter isolation |
| Gemini streaming edge cases | Explicit endpoint + event format pinned |

---

## 14) Follow-ups

PR-05 will:
- Call `LLMRouter` via dependency injection
- Persist `message_llm` (PK = `message_id`, not separate `id`)
- Handle idempotency
- Enforce budgets and limits
- Add SSE framing for HTTP responses

**PR-04 must remain stateless, pure, and reusable.**
