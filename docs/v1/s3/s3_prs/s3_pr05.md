# PR-05 Spec — Send Message Endpoint (LLM Execution, Idempotency, Rate Limits)

This PR implements the core "send message" flow for Nexus: creating user messages, executing LLM calls (OpenAI / Anthropic / Gemini), handling quote-to-chat, enforcing rate limits and platform token budgets, and finalizing assistant responses — without holding database transactions during external calls.

This PR MUST NOT introduce:

- Semantic search
- Summarization
- Message editing or branching
- Library sharing UI
- Media deletion

This PR MUST preserve all invariants defined in the Constitution, S3 Spec, and PR-01–PR-04.

---

## 1) Goals

### Primary

- Allow a user to send a message (with or without context) and receive an assistant response.
- Support platform key + BYOK resolution.
- Enforce idempotency, rate limits, and token budgets.
- Support non-streaming and streaming execution (feature-flagged).

### Secondary

- Ensure partial failures are visible and recoverable.
- Prevent duplicate or interleaved assistant messages.
- Preserve strict ordering (`seq`) and immutability guarantees.

---

## 2) Endpoint

```
POST /conversations/{conversation_id?}/messages
```

### Query Params

- `stream=1` (optional): enable SSE streaming response.

### Headers

- `Authorization: Bearer <supabase_token>`
- `Idempotency-Key: <uuid>` (optional but strongly recommended)

---

## 3) Request Body

```json
{
  "content": "User message text",
  "model_id": "uuid-of-model",
  "key_mode": "auto",
  "contexts": [
    { "type": "highlight", "id": "uuid" },
    { "type": "media", "id": "uuid" }
  ]
}
```

### Validation

- `content`: max 20,000 chars
- `contexts`: max 10 items
- `model_id`: must exist and be available to user
- `key_mode`: validated enum (`auto` | `byok_only` | `platform_only`)

Violations return 4xx with error envelope and no DB artifacts created.

---

## 4) Response Semantics (Critical Decision)

### Pre-phase failures (validation, auth, quota, availability)

- Non-2xx response
- Error envelope only
- No messages created

### Post-commit LLM failures

- Always 200
- Assistant message returned with:
  - `status = "error"`
  - `error_code`
  - `content` = user-friendly error message (see Section 15)
- No error envelope

This guarantees idempotency safety and debuggability.

---

## 5) Schema Additions

### Required Columns (if not already present)

```sql
message.error_code TEXT NULL
```

Used only when `status = 'error'`.

### Pending Assistant Uniqueness (New Index)

```sql
CREATE UNIQUE INDEX uix_one_pending_assistant_per_conversation
ON message (conversation_id)
WHERE role = 'assistant' AND status = 'pending';
```

**Why:** Service-layer checks are brittle. This turns the "at most one pending assistant per conversation" assumption into a physical invariant that survives refactors. Violation raises `IntegrityError` → `E_CONVERSATION_BUSY`.

---

## 6) Execution Model (Three-Phase)

### Phase 0 — Pre-Validation (No DB Writes)

Fail fast if any condition fails:

- Model not available → `E_MODEL_NOT_AVAILABLE`
- No usable API key → `E_LLM_NO_KEY`
- Message length > 20k → `E_MESSAGE_TOO_LONG`
- Context > limits → `E_CONTEXT_TOO_LARGE`
- **Context not visible** → `E_NOT_FOUND` (see Section 16)
- Rate limit exceeded → `E_RATE_LIMITED` (user RPM limit)
- Platform token budget exceeded → `E_TOKEN_BUDGET_EXCEEDED`
- Conversation already has pending assistant → `E_CONVERSATION_BUSY`

---

### Phase 1 — Prepare (Single DB Transaction)

**Steps:**

1. Create conversation if `conversation_id` is null.
2. Lock conversation row (`SELECT ... FOR UPDATE`).
3. Assign seq via `conversation.next_seq`.
4. Insert user message (`status=complete`).
5. Insert `message_context` rows.
6. Upsert `conversation_media`.
7. Insert assistant placeholder:
   - `role=assistant`
   - `status=pending`
   - empty content
8. Insert idempotency row:
   - `(user_id, key)` PK
   - `payload_hash`
   - `user_message_id`
   - `assistant_message_id`
   - `expires_at = now() + 24h`
9. Commit.

**Invariants:**
- Pending assistant must be the last message.
- Partial unique index `uix_one_pending_assistant_per_conversation` enforces uniqueness.

---

### Phase 2 — Execute (No DB Transaction Held)

1. Resolve API key (BYOK / platform).
2. Render prompt:
   - System prompt v1
   - Context blocks (`fragment_block` preferred, fallback ±600 chars)
3. Call adapter:
   - Non-streaming: `generate()`, timeout 45s
   - Streaming: `generate_stream()`, inactivity timeout 45s
4. Record start time.

**If provider fails:**

- Capture normalized error class (`LLMErrorClass`)
- Continue to Phase 3

---

### Phase 3 — Finalize (Single DB Transaction)

1. Update assistant message:
   - `status = complete | error`
   - `content`:
     - On success: assistant response (truncated if > 50k chars, see Section 14)
     - On error: user-friendly message (see Section 15)
   - `error_code` if error
2. Insert `message_llm` row:
   - `provider`, `model_name`
   - token usage (nullable)
   - `latency_ms`
   - `key_mode_requested`, `key_mode_used`
   - `error_class` if any
3. Update BYOK status (`valid` / `invalid`).
4. **Decrement platform token budget** (if applicable, see Section 17).
5. Commit.

---

## 7) Idempotency

### Storage

- PK: `(user_id, key)`
- Expiry: 24 hours

### Behavior

| Scenario | Behavior |
|----------|----------|
| Same key + same payload hash | Return existing messages (see below) |
| Same key + different payload hash | `E_IDEMPOTENCY_KEY_REPLAY_MISMATCH` (409) |
| Expired row | Delete and treat as new request |

### Replay Response When Assistant Still Pending

If replay hits an existing idempotency row:

- Return `conversation` + `user_message` + `assistant_message`
- Assistant message may be:
  - `status = pending` → client should poll or wait for SSE
  - `status = complete` → done
  - `status = error` → done
- **Never re-trigger Phase 2** — this prevents double execution

No explicit locks required (`ON CONFLICT DO NOTHING`).

---

## 8) Rate Limiting & Budgets (Redis)

### Required Redis Client

- Sync Redis client added to FastAPI lifespan.
- If Redis unavailable:
  - Fail closed for platform token budget
  - Fail open for RPM / concurrency

### Limits

| Limit | Default | Notes |
|-------|---------|-------|
| Requests per minute per user | 20/min | `E_RATE_LIMITED` if exceeded |
| Concurrent in-flight sends per user | 3 | `E_RATE_LIMITED` if exceeded |
| Platform key daily token budget | 100k tokens/day | `E_TOKEN_BUDGET_EXCEEDED` if exceeded |

### Error Code Taxonomy

| Code | When |
|------|------|
| `E_RATE_LIMITED` | User exceeds RPM or concurrency limit |
| `E_LLM_RATE_LIMIT` | Provider returns 429 (captured in `error_class`) |
| `E_TOKEN_BUDGET_EXCEEDED` | User's daily platform token budget exceeded |

### Token Accounting

- Use provider usage if available.
- If missing, estimate: `ceil(chars / 4)`.

BYOK users bypass platform budget.

---

## 9) Streaming (Feature-Flagged)

### Protocol

Server-Sent Events (`text/event-stream`)

### Events

```
event: meta
data: { conversation_id, user_message_id, assistant_message_id, model_id, provider }

event: delta
data: { delta: "text chunk" }

event: done
data: { status, usage?, error_code? }
```

### Rules

- No partial DB writes.
- Finalization happens once at stream end.
- Streaming disabled by default unless feature flag enabled.

---

## 10) Concurrency Rules

- A conversation may have at most one pending assistant.
- New sends rejected while pending exists (`E_CONVERSATION_BUSY`).
- Multiple conversations per user allowed concurrently (subject to limits).
- Partial unique index `uix_one_pending_assistant_per_conversation` provides physical enforcement.

---

## 11) Error Codes (PR-05 Additions)

| Code | HTTP | Meaning |
|------|------|---------|
| `E_CONVERSATION_BUSY` | 409 | Pending assistant already exists |
| `E_TOKEN_BUDGET_EXCEEDED` | 429 | Platform token budget exceeded |
| `E_RATE_LIMITED` | 429 | Per-user rate limit (RPM or concurrency) exceeded |
| `E_IDEMPOTENCY_KEY_REPLAY_MISMATCH` | 409 | Idempotency key reused with different payload |

---

## 12) Tests (Mandatory)

### Happy Path

- Message send → assistant complete
- Quote-to-chat with highlight context
- Platform key + BYOK resolution
- Non-streaming and streaming modes

### Failure Cases

- Invalid key → assistant error message with user-friendly content
- Timeout → assistant error message
- Rate limit hit → no messages created, 429
- Budget exceeded → no messages created, 429

### Idempotency

- Replay same key → same result
- Replay same key while assistant pending → returns pending assistant, no re-execution
- Replay different payload → 409
- Expired key reused → new messages

### Concurrency

- Two concurrent sends → one rejected with `E_CONVERSATION_BUSY`
- Seq strictly increasing

### Context Visibility

- Context with invisible media → `E_NOT_FOUND`, no messages created
- Context with visible highlight → success

### Streaming (if enabled)

- Chunks arrive incrementally
- Final assistant message persisted once

### Crash Recovery

- **Crash after Phase 1 commit, before Phase 2 starts:**
  - Assistant stays `status=pending`
  - Cleanup task (PR-09) converts to `status=error` after 5 min
- **Crash during Phase 3 commit:**
  - Transaction rollback leaves assistant `status=pending`
  - Cleanup task fixes it

---

## 13) Non-Goals

- No message editing
- No summarization
- No semantic search
- No retries inside same request
- No background LLM execution

---

## 14) Assistant Content Length Limit

**Problem:** Runaway model output can exceed DB limits, blow frontend rendering, and spike token costs.

**Rule:**
- Max assistant content: **50,000 chars**
- If exceeded:
  - Truncate at 50,000 chars
  - Append `"\n\n[Response truncated due to length]"`

**Implementation:** In Phase 3, before updating assistant message content.

---

## 15) Error Message Mapping (User-Friendly Assistant Content)

When `status = 'error'`, the assistant's `content` field contains a user-friendly message.

| `error_class` | Assistant `content` |
|---------------|---------------------|
| `E_LLM_TIMEOUT` | "The model timed out while responding. Please try again." |
| `E_LLM_RATE_LIMIT` | "The model is temporarily rate-limited. Please try again shortly." |
| `E_LLM_INVALID_KEY` | "The configured API key is invalid or has been revoked." |
| `E_LLM_PROVIDER_DOWN` | "The model provider is currently unavailable. Please try again later." |
| `E_LLM_CONTEXT_TOO_LARGE` | "The context was too large for the model. Please try with less context." |
| (other / unknown) | "An unexpected error occurred. Please try again." |

**Why:**
- Consistent UI across all error cases
- Avoids leaking internal error details
- Prevents different adapters from inventing copy

---

## 16) Context Visibility Validation

**Problem:** Contexts can include `media`, `highlight`, or `annotation`. All must be visible to the user.

**Pre-validation rule (Phase 0):**

1. For each context item:
   - `type=media` → `media_id = context.id`
   - `type=highlight` → `media_id = highlight.fragment.media_id`
   - `type=annotation` → `media_id = annotation.highlight.fragment.media_id`
2. Apply `can_read_media(viewer_id, media_id)` to each resolved `media_id`.
3. If **any** fail → `E_NOT_FOUND` (404, not 403 — prevents existence leaks).

**Note:** For v1, PDF highlight contexts are not supported (highlights require `fragment_id`, PDFs don't use fragments).

---

## 17) Platform Token Budget Idempotency

**Problem:** If Phase 2 fails after consuming tokens (provider-side), client retry with same idempotency key must not double-charge budget.

**Rule:** Budget decrement happens **once**, during Phase 3 finalization.

**Implementation:**

```python
# In Phase 3, after getting token count:
budget_key = f"budget:{user_id}:{date}"
charge_key = f"budget_charged:{assistant_message_id}"

# Only charge if not already charged (idempotent)
if not redis.exists(charge_key):
    redis.incrby(budget_key, tokens_used)
    redis.expire(budget_key, 86400)  # 24h TTL
    redis.set(charge_key, "1", ex=86400)  # Mark as charged
```

This ensures:
- Budget is charged exactly once per assistant message
- Retries (via idempotency) don't double-charge
- Phase 3 retries don't double-charge

---

## 18) Exit Criteria

PR-05 is complete when:

- Send-message works end-to-end (non-streaming and streaming).
- All invariants hold under failure.
- Rate limits and budgets are enforced.
- Idempotency prevents duplicates and double-execution.
- Context visibility is validated.
- No DB transaction is held during LLM calls.
- Partial unique index prevents duplicate pending assistants.
- Crash recovery tests pass.
