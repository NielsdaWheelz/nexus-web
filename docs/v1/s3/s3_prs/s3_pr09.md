# PR-09 Spec — Observability, Structured Logging, Redaction, and Streaming Diagnostics (S3)

This PR formalizes operational observability for Slice 3.

It does **not** introduce Prometheus, OpenTelemetry, or external metrics infrastructure.
It standardizes structured logging, safe redaction, streaming diagnostics, and operational invariants using:

- structlog (JSON)
- ContextVars
- Redis (existing)
- MessageLLM table (existing)

This PR must not change any user-facing behavior.

---

# 1) Goals

## Primary
- Make all LLM traffic observable, debuggable, and auditable.
- Make streaming flows diagnosable in production.
- Prevent accidental leakage of prompts, API keys, or sensitive content.
- Standardize structured log schema across the codebase.

## Secondary
- Enable future metrics backfill from logs.
- Provide clear event taxonomy for long-term metrics integration.
- Detect invariant violations (orphaned pending messages, double-finalize, JTI replay, etc).

---

# 2) Non-Goals

- No Prometheus /metrics endpoint.
- No OpenTelemetry.
- No tracing infrastructure.
- No dashboarding system.
- No user-visible changes.
- No schema changes (except `message_llm.provider_request_id` — see §7.4).

---

# 3) Design Principles

1. **Logs are the metrics system.**
2. **Never log sensitive content.**
3. **Events > prose.**
4. **Emit stable, machine-readable fields.**
5. **Never sample errors.**
6. **Streaming must be first-class observable.**
7. **Event names are the primary key — never rename events without a deprecation window.**

---

# 4) Logging Schema Standardization

All log events must follow this structure:

```json
{
  "event": "string",
  "level": "info|warning|error",
  "request_id": "...",
  "user_id": "...",
  "route_template": "/conversations/{id}/messages",
  "path": "/conversations/abc-123/messages",
  "method": "POST",
  "...": "event-specific fields"
}
```

## 4.1 Context Scoping

Not all events have the same required fields. Context depends on scope.

### Request-scoped events (HTTP handlers)

Required:
- `request_id`
- `user_id` (after auth; omitted for unauthenticated failures)
- `route_template` (preferred; see §5)
- `path` (raw path, always present as fallback)
- `method`

### Task-scoped events (Celery workers)

Required:
- `task_id`
- `task_name`
- `request_id` (when propagated from enqueuing request; nullable)
- `user_id` (when available)

### LLM events (§7)

Always required:
- `provider`
- `model_name`
- `key_mode`
- `streaming` (bool)
- `llm_operation` — enum: `chat_send` | `key_test` | `other`

Required **only when `llm_operation == chat_send`**:
- `conversation_id`
- `assistant_message_id`
- `flow_id` (see §4.2)

Optional but recommended:
- `latency_ms`
- `tokens_input`
- `tokens_output`
- `tokens_total`
- `cost_usd_micros`
- `error_class`
- `outcome` — enum: `success` | `error` | `client_disconnect`
- `provider_request_id` (see §7.4)

### Streaming events (§8)

Additional fields (on top of LLM event fields):
- `stream_jti`
- `ttft_ms` (only on `stream.first_delta`)
- `chunks_count` (only on terminal events: `stream.completed`, `stream.finalized_error`, `stream.client_disconnected`)

### Timing fields

Use the correct field name for the context:
- `latency_ms` — provider call round-trip (LLM adapter → response)
- `duration_ms` — full operation time (only on completed/terminal events and access logs)
- `elapsed_ms` — partial timing for sub-phases (phase1_db, phase2_provider, etc.)

Do not require `duration_ms` on every event. It does not exist on "started" events and is meaningless on point-in-time events.

## 4.2 flow_id — Phase Correlation

Generate `flow_id = uuid4()` at phase 0 (entry to `send_message` or `stream_send_message_async`).

Thread it through:
- All phase 1 logs (prepare)
- All phase 2 logs (LLM router calls)
- All phase 3 logs (finalize)
- All streaming generator logs
- Sweeper logs (when sweeper finalizes, log the `assistant_message_id`; `flow_id` is only available for the originating request)

`flow_id` is **not** stored in the database. It lives in memory for the duration of a single request/stream lifecycle. For cross-process correlation (e.g., sweeper cleaning up after a crashed stream), use `assistant_message_id` as the join key.

Implementation: add `flow_id_var: ContextVar[str | None]` to `nexus/logging.py`, inject via `add_request_context` processor. Set at phase 0, clear in finally.

## 4.3 Path Fields

Two separate fields for path:

| Field | Value | Example |
|---|---|---|
| `route_template` | FastAPI route path template, if available | `/conversations/{conversation_id}/messages` |
| `path` | Raw request path (no query string, ever) | `/conversations/abc-123/messages` |

Rules:
- **Never log query string.** Strip it before setting context.
- `route_template` extraction requires the request to have been matched to a route. In `RequestIDMiddleware` (which runs before routing), only `path` is available. After routing, enrich with `route_template`.
- If route matching fails (404), `route_template` will be null. That's fine.

Implementation approach: set `path` and `method` in `RequestIDMiddleware` on entry. Add `route_template` enrichment at the start of each route handler (or via a lightweight dependency). Accept that some early-middleware logs will have `path` but not `route_template`.

---

# 5) Middleware Enhancements

## 5.1 Extend ContextVars

Add to `nexus/logging.py`:
- `path_var: ContextVar[str | None]`
- `method_var: ContextVar[str | None]`
- `route_template_var: ContextVar[str | None]`
- `flow_id_var: ContextVar[str | None]`

Update `add_request_context` processor to inject all of them.

Update `set_request_context` signature:

```python
def set_request_context(
    request_id: str | None,
    user_id: str | None = None,
    path: str | None = None,
    method: str | None = None,
) -> None:
```

Add separate setter for route template (called after routing):

```python
def set_route_template(template: str | None) -> None:
    route_template_var.set(template)
```

## 5.2 RequestIDMiddleware Changes

On request entry:
1. Set `request_id` (existing)
2. Set `path` = `request.url.path` (strip query string — `url.path` already excludes it in Starlette)
3. Set `method` = `request.method`

On response (after `call_next`):
1. Retrieve `user_id` from `request.state.viewer` (existing)
2. Update context with `user_id` (existing)

Clear all context in `finally` (extend `clear_request_context` to cover new vars).

**Middleware ordering constraint:** `RequestIDMiddleware` runs before routing, so `route_template` is NOT available here. That's acceptable. It will be set downstream.

---

# 6) Redaction, Hashing, and Log Guards

Create: `nexus/services/redact.py`

## 6.1 Redaction Utilities

```python
def hash_text(value: str) -> str:
    """SHA-256 hex digest."""

def redact_text(value: str, keep: int = 0) -> str:
    """Return '***' or partial prefix + '***'."""
```

## 6.2 Log Guard — `safe_kv()`

```python
FORBIDDEN_KEYS = frozenset({
    "prompt", "content", "query", "api_key", "bearer",
    "token", "secret", "password", "message_text",
    "transcript", "context_text", "raw_body",
})

REDACTED_SUFFIXES = ("_sha256", "_hash", "_length", "_chars")

def safe_kv(**kwargs) -> dict:
    """Validate that no forbidden keys are present unless already redacted.

    Raises ValueError (in dev/test) or logs warning (in prod) if a forbidden
    key is used without a redacted suffix.

    Usage:
        logger.info("llm.request.started", **safe_kv(
            provider="openai",
            model_name="gpt-4",
            message_chars=1234,       # OK: _chars suffix
            prompt_sha256="abc123",   # OK: _sha256 suffix
            # prompt="hello world",   # BLOCKED: forbidden key
        ))
    """
```

This catches accidental logging of sensitive data at development time, not just at review time.

## 6.3 Never-Log Policy

Never log:
- API keys (plaintext or decrypted)
- Bearer tokens
- Rendered prompts
- Message content
- Transcript text
- Raw search queries
- Context block text

Allowed:
- Length of text (suffix: `_chars`, `_length`)
- SHA-256 of text (suffix: `_sha256`)
- Token counts
- Cost
- Provider request ID

All new logging code must use `safe_kv()` for keyword arguments.

## 6.4 Testing Redaction

Tests must capture actual log output and assert on it:

```python
# In pytest conftest or test setup:
import structlog

@pytest.fixture
def captured_logs():
    """Configure structlog to capture logs into a list for assertion."""
    log_output = []

    def test_processor(logger, method_name, event_dict):
        log_output.append(event_dict)
        raise structlog.DropEvent  # Don't actually emit

    structlog.configure(
        processors=[test_processor],
        wrapper_class=structlog.stdlib.BoundLogger,
    )
    yield log_output
```

Assertions must check serialized JSON strings for absence of:
- The literal prompt text used in the test
- Key-like prefixes: `sk-`, `AIza`, key material patterns
- User message substrings used in the test

Example:
```python
def test_no_prompt_in_logs(captured_logs):
    secret_prompt = "Tell me about quantum computing"
    # ... trigger LLM flow with secret_prompt ...
    serialized = json.dumps(captured_logs)
    assert secret_prompt not in serialized
    assert "sk-" not in serialized
```

---

# 7) LLM Observability

All LLM calls pass through:
- `LLMRouter.generate()`
- `LLMRouter.generate_stream()`

These are the only chokepoints.

## 7.1 `llm_operation` Enum

```python
class LLMOperation(str, Enum):
    CHAT_SEND = "chat_send"
    KEY_TEST = "key_test"
    OTHER = "other"
```

The caller passes `llm_operation` when invoking the router (or it's inferred from call site). This controls which fields are required in the log event.

- `chat_send`: requires `conversation_id`, `assistant_message_id`, `flow_id`
- `key_test`: no conversation/message context
- `other`: future non-chat LLM usage

## 7.2 Standard Events

### Event: `llm.request.started`

Fields:
- `provider`
- `model_name`
- `key_mode`
- `streaming` (bool)
- `llm_operation`
- `conversation_id` (if `chat_send`)
- `assistant_message_id` (if `chat_send`)
- `flow_id` (if `chat_send`)
- `message_chars`
- `context_chars`
- `num_context_items`

Level: `info`

---

### Event: `llm.request.finished`

Fields:
- `provider`
- `model_name`
- `key_mode`
- `streaming` (bool)
- `llm_operation`
- `outcome`: `success`
- `latency_ms`
- `tokens_input`
- `tokens_output`
- `tokens_total`
- `cost_usd_micros`
- `provider_request_id` (if available)
- `conversation_id` (if `chat_send`)
- `assistant_message_id` (if `chat_send`)
- `flow_id` (if `chat_send`)

Level: `info`

---

### Event: `llm.request.failed`

Fields:
- `provider`
- `model_name`
- `key_mode`
- `streaming` (bool)
- `llm_operation`
- `outcome`: `error`
- `error_class`
- `latency_ms`
- `provider_request_id` (if available — some errors return headers)
- `conversation_id` (if `chat_send`)
- `assistant_message_id` (if `chat_send`)
- `flow_id` (if `chat_send`)

Level: `error`

Note: Even though HTTP returns 200 for non-streaming LLM failures (error embedded in message status), this must be logged at `error` level.

## 7.3 `outcome` Field

Every terminal LLM/streaming event must include `outcome`:

| Value | Meaning |
|---|---|
| `success` | LLM call completed, response stored |
| `error` | LLM call failed (provider error, timeout, etc.) |
| `client_disconnect` | Client aborted the stream before completion |

This disambiguates from HTTP status (which is always 200 for LLM failures) and from log level (which may be `info` or `error`).

## 7.4 Provider Request ID

High-leverage debugging field. Each provider returns a request ID in response headers:

| Provider | Header |
|---|---|
| OpenAI | `x-request-id` |
| Anthropic | `request-id` |
| Gemini | Best-effort; skip if unavailable |

Implementation:
- Adapters extract the header from the HTTP response and include it in `LLMResponse` / final `LLMChunk` as `provider_request_id: str | None`.
- Router logs it on `llm.request.finished` and `llm.request.failed`.
- Store in `message_llm.provider_request_id` (new nullable column). This is the **one schema change** in this PR.

If schema changes are refused: log it only, do not add the column.

---

# 8) Streaming Observability

Streaming adds additional diagnostics on top of the LLM events.

## 8.1 Stream Context

Thread `stream_jti` (JWT ID from stream token) into logging context.

After token verification in `/stream/*` route dependencies:
- Extract `jti` from verified token claims
- Store in `ContextVar stream_jti_var`
- Inject via `add_request_context` processor

Implementation: `verify_stream_token()` already decodes the token. Return the `jti` alongside `user_id` so the route dependency can set it.

## 8.2 Streaming-Specific Events

### Event: `stream.started`

Fields:
- `assistant_message_id`
- `provider`
- `model_name`
- `stream_jti`
- `flow_id`

Level: `info`

---

### Event: `stream.first_delta`

Emitted **exactly once** per stream, on the first non-empty delta chunk.

Fields:
- `assistant_message_id`
- `ttft_ms` (time from `stream.started` to first delta)
- `provider`
- `model_name`

Level: `info`

---

### Event: `stream.completed`

Emitted on successful stream completion.

Fields:
- `assistant_message_id`
- `duration_ms`
- `chunks_count` (number of SSE `delta` events yielded — not provider chunks)
- `tokens_total`
- `cost_usd_micros`
- `outcome`: `success`
- `flow_id`
- `provider_request_id`

Level: `info`

---

### Event: `stream.client_disconnected`

Emitted when `asyncio.CancelledError` or `GeneratorExit` is caught.

Fields:
- `assistant_message_id`
- `duration_ms`
- `chunks_count`
- `outcome`: `client_disconnect`
- `flow_id`

Level: `warning`

---

### Event: `stream.finalized_error`

Emitted when stream ends due to LLM/internal error.

Fields:
- `assistant_message_id`
- `error_class`
- `duration_ms`
- `chunks_count`
- `outcome`: `error`
- `flow_id`
- `provider_request_id` (if available)

Level: `error`

## 8.3 Chunk Definition

A "chunk" is an **SSE `delta` event** yielded to the client (i.e., a call to `format_sse_event("delta", ...)`), not a provider-level chunk. We control SSE delta emission; provider chunking is an implementation detail.

`chunks_count` is only logged on terminal events (`stream.completed`, `stream.finalized_error`, `stream.client_disconnected`), never per-chunk.

---

# 9) Phase-Level Timing

Instrument the send flow with sub-phase timing.

## Non-streaming

### Event: `send.completed`

Fields:
- `flow_id`
- `conversation_id`
- `assistant_message_id`
- `outcome`
- `phase1_db_ms`
- `phase2_provider_ms`
- `phase3_finalize_ms`
- `total_ms`

Level: `info` (or `error` if `outcome == error`)

## Streaming

### Event: `stream.phases`

Logged alongside the terminal stream event.

Fields:
- `flow_id`
- `phase1_db_ms`
- `provider_stream_duration_ms`
- `finalize_ms`

---

# 10) Operational Invariant Monitoring

Emit structured events when invariants are violated.

## 10.1 Pending Sweeper

When the Celery sweeper finalizes orphaned pending messages.

Covers **both**:
- Streaming orphans (no liveness marker)
- Non-streaming orphans (stale pending messages from crashed non-streaming requests)

If this PR only instruments the streaming sweeper, explicitly state that non-streaming orphan cleanup is out of scope and tracked separately.

### Event: `sweeper.orphaned_pending_finalized`

Fields:
- `assistant_message_id`
- `age_seconds`
- `origin` — `streaming` | `non_streaming` | `unknown`

Level: `warning`

## 10.2 Double Finalize Attempt

Deterministic condition: `_finalize_stream_conditional()` returns `rowcount == 0` (the `WHERE status = 'pending'` matched nothing).

### Event: `stream.double_finalize_detected`

Fields:
- `assistant_message_id`
- `attempted_status` — the status we tried to set (`complete` or `error`)
- `reason`: `status_not_pending` (the row was already finalized by sweeper or another path)

Level: `error`

Note: Do NOT query for `previous_status` — the point of conditional update is to avoid extra reads. The reason is deterministic from `rowcount == 0`.

## 10.3 JTI Replay

If Redis SETNX fails during stream token verification.

### Event: `stream.jti_replay_blocked`

Fields:
- `jti` (safe to log — it's an opaque UUID, not a secret)

Level: `warning`

## 10.4 Idempotency Mismatch

### Event: `idempotency.replay_mismatch`

Fields:
- `idempotency_key` (hash it if you want, but it's user-provided and opaque)
- `viewer_id`

Level: `warning`

---

# 11) Rate Limits & Token Budget Observability

Emit events when enforcement triggers.

### Event: `rate_limit.blocked`

Fields:
- `user_id`
- `route_template` (NOT raw path — avoids cardinality explosion from path parameters)
- `limit_type` — `rpm` | `concurrent`

Level: `warning`

---

### Event: `token_budget.exceeded`

Fields:
- `user_id`
- `model_name`
- `provider`
- `key_mode` — always present; should never fire for `byok_only` (if it does, that's a bug worth alerting on)

Level: `warning`

---

# 12) HTTP Access Log

### Event: `http.request.completed`

This is the existing access log emitted by `RequestIDMiddleware`. Rename from `request_completed` to follow the event taxonomy.

Fields (all auto-injected via context + explicit):
- `request_id`
- `user_id`
- `method`
- `path`
- `route_template`
- `status_code` (HTTP status)
- `duration_ms`

Level: `info`

---

# 13) Event Taxonomy

All events use dotted namespace prefixes. This is the canonical list.

| Prefix | Scope |
|---|---|
| `http.request.*` | HTTP access logs |
| `llm.request.*` | LLM router calls (started/finished/failed) |
| `send.*` | Non-streaming send-message phases |
| `stream.*` | Streaming lifecycle |
| `sweeper.*` | Background cleanup jobs |
| `rate_limit.*` | RPM/concurrency enforcement |
| `token_budget.*` | Token budget enforcement |
| `idempotency.*` | Idempotency key handling |

Rules:
- Event names are stable identifiers. Never rename without a deprecation window.
- New events must use existing prefixes or propose a new prefix in the PR description.
- Consumers (future dashboards, alerting) will key on event names.

---

# 14) Logging Levels Policy

| Condition | Level |
|---|---|
| Normal request success | `info` |
| LLM success | `info` |
| LLM failure | `error` |
| Client disconnect | `warning` |
| Provider rate limit | `warning` |
| Token budget exceeded | `warning` |
| Internal exception | `error` |
| Invariant violation (double finalize) | `error` |
| Invariant violation (orphan, JTI replay) | `warning` |

Errors are never sampled.

---

# 15) Sampling Policy

- No sampling of error-level logs.
- Success logs may be sampled in future (feature-flag ready).
- Health check endpoints may be sampled or suppressed.

---

# 16) Tests

## 16.1 Test Infrastructure

Configure structlog with a test sink that captures log events as dicts:

```python
@pytest.fixture
def log_sink():
    """Capture structlog events into a list for assertion."""
    events = []
    def capture(logger, method_name, event_dict):
        events.append(event_dict.copy())
        raise structlog.DropEvent
    # configure structlog with capture processor
    yield events
```

All log-related tests use this fixture to assert on actual emitted events.

## 16.2 Unit Tests

- `redact_text` never returns full input for inputs > `keep` length
- `hash_text` is stable (same input → same output)
- `safe_kv` raises/warns on forbidden keys (`prompt`, `content`, `api_key`, etc.)
- `safe_kv` allows keys with redacted suffixes (`prompt_sha256`, `message_chars`)
- `LLMRouter.generate()` emits `llm.request.started` and `llm.request.finished` with correct fields
- `LLMRouter.generate()` emits `llm.request.failed` on adapter failure with `outcome=error`
- Streaming emits `stream.first_delta` exactly once
- Streaming emits `stream.completed` with `chunks_count` on success
- JTI is extracted and present in stream event logs
- `flow_id` is present on all phase logs within a single request
- **No logs contain plaintext prompt or API key** — assert by:
  - Capturing all log events during a test LLM flow
  - Serializing to JSON
  - Asserting the literal prompt text is absent
  - Asserting `sk-`, `AIza` patterns are absent

## 16.3 Integration Tests

- Streaming disconnect emits `stream.client_disconnected` with `outcome=client_disconnect`
- Sweeper emits `sweeper.orphaned_pending_finalized`
- Idempotency mismatch emits `idempotency.replay_mismatch`
- Rate limit emits `rate_limit.blocked` with `route_template` (not raw path)
- Double finalize emits `stream.double_finalize_detected` with `reason=status_not_pending`

---

# 17) Acceptance Criteria

- All LLM traffic logs `llm.request.started` / `llm.request.finished` / `llm.request.failed`
- All terminal events include `outcome` field (`success` | `error` | `client_disconnect`)
- Streaming logs include `ttft_ms` (on `stream.first_delta`) and `chunks_count` (on terminal events only)
- `chunks_count` counts SSE delta events, not provider chunks
- No plaintext prompt appears in any log (verified by test sink assertions)
- API keys never logged (verified by test sink assertions)
- `path` and `method` appear on all request-scoped logs via context injection
- `route_template` present when route matching succeeded; null otherwise
- Query strings never logged
- `flow_id` correlates all phase logs within a single send-message lifecycle
- `llm_operation` distinguishes `chat_send` from `key_test` — conversation/message IDs only required for `chat_send`
- Request-scoped events include `request_id` + `user_id`; task-scoped events include `task_id` + `task_name`
- `provider_request_id` captured from adapter response headers and logged on finished/failed events
- `provider_request_id` stored in `message_llm` table (nullable column) — or log-only if schema change refused
- Invariant violations emit structured events with deterministic conditions
- Rate limit / token budget events use `route_template`, not raw path
- Token budget events include `key_mode`
- `safe_kv()` guard used in all new logging call sites
- No change in user-facing behavior

---

# 18) Long-Term Compatibility

This event schema must be stable.

Future metrics infrastructure (Prometheus, OTEL) will consume:
- `latency_ms`
- `tokens_total`
- `error_class`
- `provider`
- `model_name`
- `key_mode`
- `outcome`
- `llm_operation`

Event names are the primary key for downstream consumers. Renaming an event is a breaking change.

This PR ensures migration to a full metrics stack later requires minimal code changes.

---

# 19) Risks

| Risk | Mitigation |
|---|---|
| Logging too verbose | Event-based, not per-chunk. `chunks_count` only on terminal events. |
| Accidental sensitive logging | `safe_kv()` guard + `redact()` utility + test sink assertions |
| Cardinality explosion | No raw user text, no raw URLs, `route_template` over raw path |
| Performance overhead | Logging minimal, JSON serialization only |
| `route_template` unavailable in middleware | Accept raw `path` as fallback; enrich downstream |
| `provider_request_id` missing for some providers | Nullable; best-effort extraction |

---

# 20) Final Constraint

Observability must not:
- Block request path
- Hold DB transactions
- Modify execution flow
- Leak sensitive data

All instrumentation must be side-effect-free.
