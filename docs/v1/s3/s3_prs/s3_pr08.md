# slice 3 — pr-08 spec (l3) — streaming hardening

streaming exists. pr-08 makes it reliable: fix bugs, handle disconnects, harden budgets, and move sse off vercel so we stop fighting proxy timeouts.

---

## 0) scope

### in scope
1. browser → fastapi direct for sse (`/stream/*`), same fastapi app
2. simple stream token auth (short-lived jwt, no payload binding)
3. cors on `/stream/*` only
4. fix openai adapter usage invariant
5. finalize-exactly-once (conditional update)
6. eliminate `done{status:"pending"}`
7. disconnect handling → finalize to error
8. pending sweeper (background cleanup)
9. token budget pre-reservation (platform key)
10. keepalive pings
11. streaming tests

### out of scope
- second fastapi app / sub-app mounting
- payload-hash-bound tokens
- method/path binding in tokens
- websocket transport
- resume-from-offset streaming
- partial content persistence during stream

---

## 1) architecture

### before (pr-07)
```
non-streaming: browser → vercel bff → fastapi → provider
streaming:     browser → vercel bff → fastapi → provider (bff proxies sse)
```

### after (pr-08)
```
non-streaming: browser → vercel bff → fastapi (unchanged)
streaming:     browser → fastapi (render) → provider (direct, no bff)
```

same fastapi process. streaming routes live under `/stream/` as a regular APIRouter — not a mounted sub-application.

why: vercel pro has a 60s function timeout. vercel edge/node can buffer unpredictably. removing the bff from the stream path eliminates an entire class of timeout/buffering bugs.

### infra requirement (must decide before implementation)
`STREAM_BASE_URL` env var defines the public url browsers use for `/stream/*` (e.g. `https://api.nexus.example.com`). this is the same render service that hosts fastapi, but:
- what sits in front of it? (nginx, cloudflare, render's own proxy, fly proxy)
- what is the idle timeout of that proxy? (must be > 5 min for slow streams)
- does it buffer response bodies? (must not, or must support `X-Accel-Buffering: no`)
- does it support HTTP/2? (affects `Connection: keep-alive` behavior)

without answering these, keepalive intervals and timeout handling are guesswork. answer before implementing.

---

## 2) stream token auth

### flow (binding: fastapi mints)
1. browser calls `POST /api/stream-token` on vercel bff (supabase session cookie auth)
2. bff proxies to fastapi `POST /internal/stream-tokens` (with `X-Nexus-Internal` header)
3. fastapi mints a signed jwt and returns `{ token, stream_base_url, expires_at }`
4. bff forwards response to browser
5. browser opens sse: `POST {stream_base_url}/stream/conversations/{id}/messages` with `Authorization: Bearer <token>`
6. fastapi `/stream/*` route verifies token

**why fastapi mints, not bff:** `STREAM_TOKEN_SIGNING_KEY` stays in fastapi's env only. putting it in vercel means trusting vercel runtime, preview deploys, teammate env access, and log config not to leak it. one extra hop at connect time (~50ms) is cheap; key containment is not optional.

### jwt claims
- `iss`: `"nexus-stream"` (distinguishes from supabase JWTs which have a different issuer)
- `aud`: `"nexus-api"`
- `sub`: user_id (uuid)
- `exp`: now + 60s
- `iat`: now
- `jti`: uuid (one-time use)
- `scope`: `"stream"`

that's it. no conversation_id, no method, no path, no payload_hash. `iss` + `aud` prevent accidental acceptance of supabase tokens on stream routes (and vice versa).

### signing
- HS256 with `STREAM_TOKEN_SIGNING_KEY` env var
- Ed25519 is fine too but HS256 is simpler and sufficient for same-server signing+verification

### verification (fastapi dependency)
- signature valid against `STREAM_TOKEN_SIGNING_KEY`
- `iss == "nexus-stream"`
- `aud == "nexus-api"`
- `exp` not passed
- `scope == "stream"`
- `jti` not seen before: `SETNX jti:{jti} 1 EX {exp - now}` in redis. SETNX fails → reject (replay)
- extract `sub` as `viewer_user_id`

requiring `iss` + `aud` makes accidental acceptance of supabase JWTs (or any other JWT) impossible even if someone misconfigures the middleware skip.

### auth middleware skip
the main app's `AuthMiddleware` (supabase jwks + `X-Nexus-Internal` header) must skip `/stream/*` paths. stream routes use `Depends(verify_stream_token)` instead.

implementation: add a path check at the top of `AuthMiddleware.dispatch()`:
```python
if request.url.path.startswith("/stream/"):
    return await call_next(request)
```

this skips both supabase jwt verification AND `X-Nexus-Internal` header enforcement for stream routes. `RequestIDMiddleware` still runs (it's outermost).

the `iss`/`aud` requirement (§2) is the safety guard: supabase JWTs have a different issuer, so they're rejected even if someone hits `/stream/*` with their regular auth header.

### rate limiting
token mint endpoint shares the same per-user rpm limit as send-message.

---

## 3) cors (streaming routes only)

**starlette's `CORSMiddleware` is not path-scoped.** once installed, it applies to all routes. do not use it.

**do NOT use `BaseHTTPMiddleware` either.** `BaseHTTPMiddleware` wraps the response body in a way that buffers `StreamingResponse` entirely before sending — this silently defeats incremental delivery on every `/stream/*` response. this is a known starlette limitation.

instead, write a **pure ASGI middleware**:

```python
class StreamCORSMiddleware:
    """Pure ASGI middleware. Does not buffer streaming responses."""

    def __init__(self, app: ASGIApp, allowed_origins: list[str]):
        self.app = app
        self.allowed_origins = set(allowed_origins)

    async def __call__(self, scope, receive, send):
        if scope["type"] != "http" or not scope["path"].startswith("/stream/"):
            await self.app(scope, receive, send)
            return

        headers = dict(Headers(scope=scope))
        origin = headers.get("origin")

        if origin is None:
            # no Origin = non-browser (curl, tests). pass through.
            await self.app(scope, receive, send)
            return

        if origin not in self.allowed_origins:
            response = Response(status_code=403, content="origin not allowed")
            await response(scope, receive, send)
            return

        if scope["method"] == "OPTIONS":
            response = Response(status_code=204, headers={
                "access-control-allow-origin": origin,
                "access-control-allow-methods": "POST, OPTIONS",
                "access-control-allow-headers": "Authorization, Content-Type, Idempotency-Key",
                "access-control-max-age": "600",
            })
            await response(scope, receive, send)
            return

        # wrap send to inject cors headers on the response
        async def send_with_cors(message):
            if message["type"] == "http.response.start":
                headers = MutableHeaders(scope=message)
                headers.append("access-control-allow-origin", origin)
                headers.append("access-control-expose-headers", "X-Request-Id")
            await send(message)

        await self.app(scope, receive, send_with_cors)
```

this passes `StreamingResponse` chunks through without buffering. only injects cors headers on the initial `http.response.start` message.

config:
- `STREAM_CORS_ORIGINS` env var: explicit origin list (e.g. `https://nexus.example.com`). **no wildcard.**
- credentials: not set (no cookies)
- all other fastapi endpoints remain cors-disabled

`Authorization` header triggers preflight. the middleware handles `OPTIONS` before any auth dependency runs.

---

## 4) streaming correctness fixes

### 4.1 openai adapter usage invariant
bug: openai adapter yields `LLMChunk(done=False, usage!=None)` mid-stream, violating the frozen dataclass `__post_init__` check.

fix:
- accumulate usage into local vars during stream
- non-terminal chunks: `LLMChunk(delta_text=..., done=False, usage=None)`
- terminal chunk: `LLMChunk(delta_text="", done=True, usage=accumulated_usage)`

### 4.2 finalize exactly-once

`_finalize_stream` can race with the sweeper. enforce at DB level:

```sql
UPDATE messages
SET content = :content, status = :status, error_code = :error_code, updated_at = :now
WHERE id = :id AND status = 'pending'
```

check `rowcount == 1`. if 0 → already finalized; skip `message_llm` insert.

`message_llm.message_id` is the primary key (verified in schema). this provides a DB-level uniqueness guard against duplicate `message_llm` inserts even in weird race paths — a second insert for the same message will fail with a PK violation.

`messages.error_code` (nullable text) already exists in the schema (line 707 of `models.py`). no migration needed.

### 4.3 eliminate `done{status:"pending"}`

the stream endpoint always returns SSE. never return JSON from an SSE endpoint — mixing response types is a client footgun.

on idempotency replay, if assistant message is `pending`:
- check redis `stream_active:{assistant_message_id}`
- if exists (stream still running): emit `meta` (with conversation_id, assistant_message_id, user_message_id from the replay lookup) then `done{status:"error", error_code:"E_STREAM_IN_PROGRESS"}`. the `meta` is required — the protocol invariant says `meta` is always first. the replay lookup already has all the IDs needed.
- if missing (orphaned): finalize to `error` via conditional update (§4.2), then emit `meta` + `done{status:"error", error_code:"E_ORPHANED_PENDING"}`

**all replay paths emit `meta` then `done`.** never emit `done` without a preceding `meta`.

**client rule for `E_STREAM_IN_PROGRESS`:** do not show a user-visible error. switch to polling `GET /api/conversations/{id}/messages` (bff, supabase auth) every 2s for up to 30s. if the assistant message completes during polling, render it. if still pending after 30s, show "message is still generating — please wait" with a manual retry button (which uses a new idempotency key).

never return bare `done{pending}`. never return non-SSE from the stream endpoint.

### 4.4 `done` schema — one optional addition
- `done` MAY include `final_chars?: int` (character count of full assistant content)
- backward-compatible; clients that don't read it are unaffected

---

## 5) disconnect handling

### goal
when the client disconnects: stop reading from provider, close the upstream connection, finalize to error.

### implementation
convert `stream_send_message` to an **async generator** (`AsyncIterator[str]`). this eliminates the daemon-thread bridge and makes cancellation natural:
- provider streaming runs in the same event loop via async httpx
- `StreamingResponse` iterates the async generator directly
- when ASGI stops iterating (disconnect), the `finally` block runs

**the async generator alone does not stop provider spend.** you must also close the httpx stream:
```python
async with client.stream("POST", url, ...) as response:
    async for chunk in response.aiter_lines():
        yield format_sse_event(...)
# exiting `async with` calls response.aclose() automatically
```
the `async with` context manager ensures the httpx stream is closed on any exit (normal, exception, or cancellation). also keep the existing 45s httpx timeout so it can't hang forever.

handle `asyncio.CancelledError` and `GeneratorExit` in the generator's `finally:` block to ensure finalize + cleanup always run.

### sync DB access in an async generator
the existing codebase uses sync sqlalchemy sessions. do **not** do sync DB work inside the async streaming loop directly — it blocks the event loop.

skeleton (the actual sequencing):

```python
async def stream_send_message_async(db_factory, viewer_id, ...) -> AsyncIterator[str]:
    db = db_factory()  # get a sync session for this request
    error = None
    content = ""
    try:
        # phase 1: sync db (prepare)
        prepare = await run_in_threadpool(phase1_prepare, db, viewer_id, ...)
        resolved_key = await run_in_threadpool(resolve_api_key, db, ...)

        # set liveness marker BEFORE first byte
        await set_liveness_marker(prepare.assistant_message.id)

        # reserve budget (platform key only)
        if resolved_key.mode == "platform":
            await reserve_token_budget(viewer_id, prepare.assistant_message.id, est)

        # yield meta (first event, always)
        yield format_sse_event("meta", {...})

        # phase 2: stream from provider (async, same event loop)
        async with httpx_client.stream("POST", provider_url, ...) as response:
            last_keepalive = time.monotonic()
            async for chunk in parse_provider_stream(response):
                content += chunk.delta_text
                yield format_sse_event("delta", {"delta": chunk.delta_text})
                await refresh_liveness_marker(prepare.assistant_message.id)

                # keepalive check
                if time.monotonic() - last_keepalive > 15:
                    yield ": keepalive\n\n"
                    last_keepalive = time.monotonic()
        # exiting async with → response.aclose() called automatically

    except Exception as e:
        error = e
    finally:
        # phase 3: finalize (sync db, never skip)
        await run_in_threadpool(_finalize_stream, db, ..., content, error)
        await clear_liveness_marker(prepare.assistant_message.id)
        rate_limiter.decrement_inflight(viewer_id)
        db.close()

    yield format_sse_event("done", {"status": "error" if error else "complete", ...})
```

this avoids migrating the db layer to async sqlalchemy. sync db calls are wrapped in `run_in_threadpool` (starlette provides this) so they don't block the event loop.

### finalize-on-disconnect
- finalize assistant as `status=error`, `error_code="E_CLIENT_DISCONNECT"`
- uses conditional update (§4.2), safe against sweeper race
- inflight counter decremented in `finally:` (already exists)
- liveness marker cleared at finalize
- budget reservation committed/released at finalize

---

## 6) stream liveness marker

- set **before first byte is yielded** (after phase 1, before streaming loop): `SETEX stream_active:{assistant_message_id} 600 "1"`
- refreshed on every delta yield and every keepalive ping (sliding TTL via `EXPIRE`)
- cleared in finalize's `finally:` block — even on cancellation/exception
- used by replay logic (§4.3) and sweeper (§7) to distinguish running vs orphaned

---

## 7) pending sweeper

celery beat job `sweep_pending_messages`:
- query: `role='assistant' AND status='pending' AND created_at < now()-5min`
- if `stream_active:{message_id}` exists in redis → skip
- finalize via conditional update (§4.2): set `error_code='E_ORPHANED_PENDING'`, content = error message
- insert `message_llm` row if none exists: `INSERT INTO message_llm (...) VALUES (...) ON CONFLICT (message_id) DO NOTHING` — safe because `message_id` is the PK, so the sweeper and the generator can't both insert
- log count + oldest age

---

## 8) token budget pre-reservation (platform key)

### problem
charging at finalize allows overspend with concurrent streams.

### reservation model
extend rate limiter with:
- `reserve_token_budget(user_id, reservation_id, est_tokens, ttl=5min)`
- `commit_token_budget(user_id, reservation_id, actual_tokens)` — decrement reserved, increment spent
- `release_token_budget(user_id, reservation_id)` — decrement reserved (early failure before provider call)

redis keys:
- `spent:{user}:{day}` — integer
- `reserved:{user}:{day}` — integer
- `reservation:{assistant_message_id}` → `est_tokens` (with TTL)

check: `spent + reserved >= budget` → reject. reservation keyed by `assistant_message_id`.

`budget` is the existing `DEFAULT_TOKEN_BUDGET` (100k tokens/day) from `rate_limit.py`. the reservation system extends the existing `RateLimiter` class — it reads `spent` from the same `budget:{user_id}:{date}` key that `check_token_budget` and `charge_token_budget` already use. no new budget concept.

### estimation formula (binding)
`est_tokens = prompt_est + output_ceiling`

- `prompt_est` = `len(system_prompt + context + user_content) // 4 + 100` (rough char/4 heuristic — deliberately pessimistic)
- `STREAM_MAX_OUTPUT_TOKENS_DEFAULT` = 1024 (env-configurable). this is the default output ceiling for reservation — not the model's hard max.
- `output_ceiling` = `min(model.max_output_tokens, requested_max_output or STREAM_MAX_OUTPUT_TOKENS_DEFAULT)`
- request body MAY include `max_output_tokens` to request more/fewer. capped at `model.max_output_tokens`.
- if model has no configured max, fall back to `STREAM_MAX_OUTPUT_TOKENS_DEFAULT`.

reserving `prompt_est + model_max_output` (e.g. 4096) for every request would reject too aggressively on platform budget. using a smaller default (1024) keeps the feature usable while still preventing unbounded spend. actual cost is reconciled at commit.

### failure modes
- redis unavailable: fail closed (`E_RATE_LIMITER_UNAVAILABLE`) + local process semaphore as degraded brake
- redis dies mid-stream: continue; reservation TTL handles cleanup
- byok: fail open (user's own key)

---

## 9) keepalive

every ~15s with no delta, emit sse comment: `: keepalive\n\n`

keeps connections alive through proxies/LBs. also refreshes liveness marker (§6).

15s is assumed safe (well under any reasonable proxy idle timeout). once infra is decided (§1), adjust to `min(15s, idle_timeout / 3)` if the actual idle timeout is known to be short.

**parser verification:** the frontend SSE parser (`sse.ts`) already ignores lines starting with `:` (comment lines per SSE spec). verified — no parser change needed for keepalive.

---

## 10) endpoints

### new

**bff:**
- `POST /api/stream-token` — proxies to fastapi `POST /internal/stream-tokens`. returns `{ token, stream_base_url, expires_at }`

**fastapi:**
- `POST /internal/stream-tokens` — mints stream token jwt (bff-only, requires `X-Nexus-Internal` header + supabase bearer). signing key never leaves fastapi env.

### modified

**fastapi streaming routes** (now browser-callable):
- `POST /stream/conversations/{id}/messages` — existing conversation
- `POST /stream/conversations/messages` — new conversation
- auth: `Authorization: Bearer <stream_token>` (not supabase)
- cors via custom middleware (§3)
- request body: `{ content: string, model_id: uuid, key_mode?: "auto"|"byok_only"|"platform_only", contexts?: ContextItem[], max_output_tokens?: int }` — same shape as the existing `SendMessageRequest` plus optional `max_output_tokens` for budget reservation (§8)

### deprecated (not immediately deleted)
- `apps/web/src/app/api/conversations/[id]/messages/stream/route.ts` (bff streaming proxy)
- `apps/web/src/app/api/conversations/messages/stream/route.ts` (bff streaming proxy)
- keep for one deploy cycle: return `410 Gone` with message `"Use /api/stream-token to get a direct streaming URL"`. delete in a follow-up once all clients are updated.

---

## 11) frontend changes

the frontend change is more than "point sse.ts at a new url". the full flow:

### 11.1 send message flow (new)
1. user clicks send
2. call `POST /api/stream-token` (bff, supabase cookie auth). on failure: show error, stop.
3. receive `{ token, stream_base_url, expires_at }`
4. open SSE: `POST {stream_base_url}/stream/conversations/{id}/messages` with `Authorization: Bearer {token}` and the full message body
5. parse events as before (meta → deltas → done)

### 11.2 token expiry retry
the token has a 60s TTL. if the SSE connection fails with 401 (token expired — slow network, backgrounded tab):
- automatically fetch a new token (step 2)
- retry the SSE connection once
- if second attempt also fails: surface the error

### 11.3 `E_STREAM_IN_PROGRESS` handling
on `done{error_code:"E_STREAM_IN_PROGRESS"}`:
- do not show a user-visible error
- extract `assistant_message_id` from the preceding `meta` event
- poll `GET /api/conversations/{id}/messages` every 2s for up to 30s
- if the assistant message appears as `complete` during polling: render it
- if still `pending` after 30s: show "message is still generating" + manual retry button (new idempotency key)

### 11.4 files changed
- `apps/web/src/lib/api/sse.ts` — accept `stream_base_url` + `token` instead of constructing a bff url. send `Authorization: Bearer` header.
- `apps/web/src/components/chat/ChatComposer.tsx` (or equivalent) — call `/api/stream-token` before opening SSE. handle token fetch errors. implement retry on 401.
- add `apps/web/src/lib/api/streamToken.ts` — thin wrapper: `fetchStreamToken(): Promise<{token, stream_base_url, expires_at}>`

---

## 12) response headers (streaming)

- `Content-Type: text/event-stream; charset=utf-8`
- `Cache-Control: no-cache, no-transform`
- `X-Accel-Buffering: no`

no `Content-Length`. no `Connection: keep-alive` (default in HTTP/1.1, ignored in HTTP/2).

---

## 13) observability

structured log at stream start + end:
- `request_id`, `viewer_user_id`, `conversation_id`, `assistant_message_id`
- `provider`, `model_id`, `key_mode`
- `ttft_ms`, `total_ms`, `chars_generated`
- `status`, `error_code`, `disconnect_detected`

do not log prompts or api keys.

---

## 14) tests

### 13.1 streaming happy path + idempotency
- `test_stream_happy_path_emits_meta_delta_done`
- `test_stream_idempotency_replay_complete_returns_single_delta_done`
- `test_stream_idempotency_replay_pending_with_liveness_returns_in_progress_error`
- `test_stream_idempotency_replay_pending_orphaned_forces_error`

### 13.2 openai usage fix
- mock openai sse with mid-stream usage → no crash, usage only on terminal chunk

### 13.3 disconnect → finalize (most important test)
- **dependency-inject** the provider stream factory (or adapter) so you can assert `aclose()` was called without real sockets or timing
- start streaming request via httpx against ASGI app
- read `meta` event
- close the response / cancel the request
- assert within 5s:
  - assistant message status = `error`, error_code = `E_CLIENT_DISCONNECT`
  - `stream_active:{assistant_message_id}` key cleared from redis
  - inflight counter decremented
  - injected mock adapter's `aclose()` was called (provider stop-spend)
  - if budget was reserved: reservation committed or released

### 13.4 finalize idempotency
- two threads race to finalize → exactly one `message_llm` row

### 13.5 stream token auth
- expired token → rejected
- replayed jti → rejected
- wrong scope → rejected
- valid token → meta/delta/done arrive

### 13.6 bff stream-token endpoint
- requires authenticated session
- returns token + stream_base_url

### 13.7 sweeper
- pending assistant >5min, no liveness marker → finalized to error

### 13.8 incremental delivery
- upstream mock emits chunk1, waits 100ms, emits chunk2
- assert chunk1 arrives before chunk2 is produced (proves no buffering)

---

## 15) acceptance criteria

1. openai streaming does not crash on mid-stream usage
2. disconnect → assistant finalized to error within 5s (not sweeper)
3. idempotency replay never returns `done{pending}`
4. platform-key streams reserve budget before provider call
5. browser connects directly to fastapi for streaming (not through bff)
6. cors on streaming: allowlist only, no cookies
7. stream token: expired/replayed/wrong-scope all rejected
8. sweeper cleans orphaned pending, skips active streams
9. keepalive comments every ~15s during idle
10. incremental delivery test passes (no buffering)

---

## 16) error codes

- `E_CLIENT_DISCONNECT` — stream aborted by client
- `E_ORPHANED_PENDING` — sweeper cleanup
- `E_STREAM_IN_PROGRESS` — replay while stream running (returned as `done{error}` via SSE)
- `E_RATE_LIMITER_UNAVAILABLE` — budget system down, fail closed
- `E_STREAM_TOKEN_EXPIRED` — token past expiry
- `E_STREAM_TOKEN_REPLAYED` — jti already used
- `E_STREAM_TOKEN_INVALID` — signature or claims failed

---

## 17) code changes

### add
| file | what |
|------|------|
| `python/nexus/auth/stream_token.py` | mint + verify stream token (HS256 jwt, iss/aud checks, jti redis SETNX) |
| `python/nexus/middleware/stream_cors.py` | custom path-scoped CORS middleware for `/stream/*` only |
| `python/nexus/routes/stream.py` | streaming APIRouter under `/stream/`, `Depends(verify_stream_token)` |
| `python/nexus/services/stream_liveness.py` | redis SETEX/refresh/clear for `stream_active:{id}` |
| `python/nexus/tasks/sweep_pending.py` | celery beat task |
| `apps/web/src/app/api/stream-token/route.ts` | bff proxies to fastapi `POST /internal/stream-tokens` |
| `python/tests/test_send_message_stream.py` | all tests from §14 |
| `python/nexus/routes/stream_tokens.py` | `POST /internal/stream-tokens` (mints token, bff-only) |
| `python/tests/test_stream_token.py` | token mint + verify tests |

### modify
| file | what |
|------|------|
| `python/nexus/auth/middleware.py` | skip supabase auth + internal header for `/stream/*` paths |
| `python/nexus/app.py` | add `StreamCORSMiddleware`, include stream router |
| `python/nexus/services/send_message_stream.py` | async generator, `async with client.stream()` for provider, `run_in_executor` for sync DB finalize, disconnect detection, liveness marker, keepalive, conditional update finalize |
| `python/nexus/services/llm/openai_adapter.py` | accumulate usage mid-stream, emit on terminal chunk only |
| `python/nexus/services/rate_limit.py` | add `reserve_token_budget`, `commit_token_budget`, `release_token_budget` |
| `python/nexus/services/send_message.py` | idempotency replay: `E_STREAM_IN_PROGRESS` via SSE for in-progress, finalize orphaned (no `done:pending`) |
| `apps/web/src/lib/api/sse.ts` | point at `stream_base_url` instead of bff, send stream token as bearer |

### deprecate (keep for one deploy, then remove)
| file | what |
|------|------|
| `apps/web/src/app/api/conversations/[id]/messages/stream/route.ts` | return 410 Gone |
| `apps/web/src/app/api/conversations/messages/stream/route.ts` | return 410 Gone |

---

## 18) implementation order

1. **decide infra**: what hostname serves `/stream/*`, what proxy sits in front, what's the idle timeout (§1)
2. stream token: mint + verify in fastapi + redis jti SETNX
3. auth middleware: skip `/stream/*` (supabase + internal header)
4. custom CORS middleware for `/stream/*`
5. stream router: move streaming routes to `/stream/` as APIRouter
6. bff: `POST /api/stream-token` (proxy to fastapi mint)
7. frontend: call stream-token, then open sse directly to fastapi
8. deprecate bff streaming proxy routes (410 Gone)
9. fix openai adapter usage invariant
10. async generator + `async with client.stream()` + `run_in_executor` for sync finalize
11. finalize conditional update (exactly-once)
12. liveness marker (set before first byte, clear in finally) + eliminate `done:pending`
13. disconnect handling → finalize to error
14. budget reservation
15. keepalive pings
16. sweeper task
17. all tests (disconnect test is the most important)
