# Nexus — L3 PR Spec

## Slice 1 · PR-01 — Infra, Migrations, CI

**Status:** ready to implement
**Depends on:** S0 fully merged
**Does not introduce extractors or enqueue jobs**

---

## 0) Goal (what this PR accomplishes)

Establish the infrastructure and schema foundation required for S1:

- redis + celery exist and boot
- database schema supports ingestion lifecycle
- CI runs real integration tests (db + redis)
- storage and job wiring can be added safely later
- no behavior change for users beyond schema expansion

After this PR:

- the repo boots consistently (local + CI)
- migrations reflect S1 spec
- workers can start (even if idle)
- future PRs can rely on the new fields/types

---

## 1) Scope (what is included)

**Included**

- docker-compose infra extension (redis added to existing)
- alembic migration for S1 schema extensions
- celery app bootstrap (no scheduling)
- CI workflow updates
- .env.example extensions
- ORM model updates for new fields

**Explicitly Excluded**

- any ingestion logic
- any celery tasks doing real work
- any scheduling/enqueueing
- any UI beyond what already exists in S0
- any storage upload/download endpoints

---

## 2) Repo Layout (binding)

This matches the existing S0 structure. Do not improvise:

```
apps/
  api/
    main.py           # thin launcher (creates app via create_app)
  web/
    src/
  worker/
    main.py           # celery app
python/
  nexus/
    db/
      models.py       # ORM models (new)
    ...
  tests/
migrations/
  alembic/
    versions/
      0001_slice0_schema.py
      0002_slice1_ingestion_framework.py  # this PR
docker/
  docker-compose.yml  # S0 has postgres; S1 adds redis
Makefile
.env.example
.github/workflows/ci.yml
```

Notes:

- `migrations/alembic/versions/` is the canonical migration location
- `apps/worker/main.py` contains celery app (no extractors)
- shared python code lives in `python/nexus/`
- models live in `python/nexus/db/models.py`

---

## 3) Infrastructure

### 3.1 docker-compose

Update `docker/docker-compose.yml` to add redis (postgres exists from S0).

**Services (final state after S1)**

- `postgres:15.8` (already exists from S0)
  - exposes port 5432 to host
  - env-configured user/db/password
- `redis:7.2-alpine` (add in S1)
  - exposes port 6379
  - no persistence required for local dev

**Requirements**

- compose is used in CI
- compose is usable for local dev
- app processes run on host (not in docker) in v1

**Image pinning note:** Pin to specific minor versions (15.8, 7.2) to avoid surprise updates. Accept patch drift.

---

## 4) Environment Configuration

### 4.1 .env.example (root)

Add the following (do not remove S0 entries):

```
# Core
NEXUS_ENV=local
NEXUS_INTERNAL_SECRET=local-dev-secret

# Database (matches docker/docker-compose.yml)
DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:5432/nexus_dev

# Redis / Celery
REDIS_URL=redis://localhost:6379/0
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0
```

**DATABASE_URL note:** Uses `postgresql+psycopg://` (SQLAlchemy 2.x + psycopg 3). Ensure psycopg[binary] is installed, not psycopg2.

**Compose match:** The db name `nexus_dev` and credentials must match docker-compose.yml POSTGRES_* env vars.

---

## 5) Database Schema (Alembic Migration)

### 5.1 Migration File

Create:

```
migrations/alembic/versions/0002_slice1_ingestion_framework.py
```

Revision chain: `0002` depends on `0001` (S0).

---

### 5.2 Enum Types (Postgres)

Create enums using guarded DO blocks (CREATE TYPE IF NOT EXISTS is not portable):

```python
def upgrade() -> None:
    # Processing status enum
    op.execute("""
        DO $
        BEGIN
            CREATE TYPE processing_status AS ENUM (
                'pending', 'extracting', 'ready_for_reading', 'embedding', 'ready', 'failed'
            );
        EXCEPTION
            WHEN duplicate_object THEN NULL;
        END $;
    """)

    # Failure stage enum
    op.execute("""
        DO $
        BEGIN
            CREATE TYPE failure_stage AS ENUM (
                'upload', 'extract', 'transcribe', 'embed', 'other'
            );
        EXCEPTION
            WHEN duplicate_object THEN NULL;
        END $;
    """)
```

**Downgrade order:** Drop columns that use the enum BEFORE dropping the type:

```python
def downgrade() -> None:
    # Drop new columns first
    op.drop_column('media', 'processing_status_v2')
    op.drop_column('media', 'failure_stage')
    # ... other columns ...

    # Drop tables
    op.drop_table('media_file')

    # Drop partial indexes
    op.drop_index('uix_media_canonical_url', table_name='media')
    op.drop_index('uix_media_file_sha256', table_name='media')

    # NOW drop types
    op.execute("DROP TYPE IF EXISTS processing_status")
    op.execute("DROP TYPE IF EXISTS failure_stage")
```

---

### 5.3 S0 processing_status Migration

S0 uses a text column with CHECK constraint for `processing_status`. S1 needs a proper enum.

**Strategy:** Add new enum column, migrate data, drop old:

```python
# 1. Add new enum column
op.add_column('media', sa.Column('processing_status_v2',
    sa.Enum('pending', 'extracting', 'ready_for_reading', 'embedding', 'ready', 'failed',
            name='processing_status', create_type=False),
    nullable=True))

# 2. Migrate data
op.execute("UPDATE media SET processing_status_v2 = processing_status::processing_status")

# 3. Drop old constraint and column
op.drop_constraint('ck_media_processing_status', 'media')
op.drop_column('media', 'processing_status')

# 4. Rename new column
op.alter_column('media', 'processing_status_v2', new_column_name='processing_status')

# 5. Set not null + default
op.alter_column('media', 'processing_status', nullable=False,
                server_default='pending')
```

---

### 5.4 media Table Extensions

Add columns to existing media table:

| Column | Type | Nullable | Default | Notes |
|--------|------|----------|---------|-------|
| `failure_stage` | failure_stage (enum) | yes | null | |
| `last_error_code` | text | yes | null | |
| `last_error_message` | text | yes | null | |
| `processing_attempts` | integer | **no** | 0 | Must be NOT NULL |
| `processing_started_at` | timestamptz | yes | null | |
| `processing_completed_at` | timestamptz | yes | null | |
| `failed_at` | timestamptz | yes | null | |
| `requested_url` | text | yes | null | Max 2048 chars |
| `canonical_url` | text | yes | null | Max 2048 chars |
| `file_sha256` | text | yes | null | |
| `external_playback_url` | text | yes | null | |
| `provider` | text | yes | null | |
| `provider_id` | text | yes | null | |
| `created_by_user_id` | uuid | **no** | null | FK to users, backfill needed |

**URL length constraints:**

```python
# Add check constraints for URL lengths
op.create_check_constraint(
    'ck_media_requested_url_length',
    'media',
    'requested_url IS NULL OR char_length(requested_url) <= 2048'
)
op.create_check_constraint(
    'ck_media_canonical_url_length',
    'media',
    'canonical_url IS NULL OR char_length(canonical_url) <= 2048'
)
```

**created_by_user_id backfill:** S0 media rows have no creator. Options:
1. Create a system user and backfill (preferred)
2. Allow null temporarily (violates spec)

Decision: Create system user during migration, backfill existing rows.

**Do not:**

- add triggers
- change existing semantics beyond processing_status migration

---

### 5.5 media_file Table

Create table:

```sql
media_file (
  media_id uuid primary key references media(id) on delete cascade,
  storage_path text not null,
  content_type text not null,
  size_bytes bigint not null  -- bigint, not integer (handles >2GB files)
)
```

**Note:** `size_bytes` uses `bigint` (not integer) to handle files >2GB.

---

### 5.6 Partial Unique Indexes

Add:

```python
# URL-based idempotency
op.create_index(
    'uix_media_canonical_url',
    'media',
    ['kind', 'canonical_url'],
    unique=True,
    postgresql_where=sa.text('canonical_url IS NOT NULL')
)

# File-based idempotency (per-user, for pdf/epub)
op.create_index(
    'uix_media_file_sha256',
    'media',
    ['created_by_user_id', 'kind', 'file_sha256'],
    unique=True,
    postgresql_where=sa.text("file_sha256 IS NOT NULL AND kind IN ('pdf', 'epub')")
)
```

These enforce S1 idempotency. The URL length CHECK constraints (§5.4) ensure the btree index doesn't exceed size limits.

---

### 5.7 Extensions (if needed)

If later slices (S3+) require pgvector:

```python
op.execute("CREATE EXTENSION IF NOT EXISTS vector")
```

**Decision:** Do NOT add in PR-01 unless S1 spec requires it. Currently S1 does not use embeddings.

---

## 6) ORM Model Updates

Create/update `python/nexus/db/models.py`:

```python
from enum import Enum as PyEnum
from sqlalchemy import Column, UUID, Text, Integer, BigInteger, Boolean, ForeignKey, Enum
from sqlalchemy.dialects.postgresql import TIMESTAMP
from sqlalchemy.orm import relationship

class ProcessingStatus(str, PyEnum):
    pending = "pending"
    extracting = "extracting"
    ready_for_reading = "ready_for_reading"
    embedding = "embedding"
    ready = "ready"
    failed = "failed"

class FailureStage(str, PyEnum):
    upload = "upload"
    extract = "extract"
    transcribe = "transcribe"
    embed = "embed"
    other = "other"

class Media(Base):
    __tablename__ = "media"
    # ... existing S0 columns ...

    # S1 additions
    processing_status = Column(Enum(ProcessingStatus), nullable=False, default=ProcessingStatus.pending)
    failure_stage = Column(Enum(FailureStage), nullable=True)
    last_error_code = Column(Text, nullable=True)
    last_error_message = Column(Text, nullable=True)
    processing_attempts = Column(Integer, nullable=False, default=0)
    # ... etc

class MediaFile(Base):
    __tablename__ = "media_file"
    media_id = Column(UUID, ForeignKey("media.id", ondelete="CASCADE"), primary_key=True)
    storage_path = Column(Text, nullable=False)
    content_type = Column(Text, nullable=False)
    size_bytes = Column(BigInteger, nullable=False)  # BigInteger for >2GB
```

**Constraints:**

- no business logic in models
- no default transitions beyond column defaults

---

## 7) Celery Bootstrap

### 7.1 Worker App

Update `apps/worker/main.py`:

```python
"""Celery worker entrypoint.

Run with: celery -A apps.worker.main worker --loglevel=info
"""
from celery import Celery
from nexus.config import settings

app = Celery('nexus')
app.conf.broker_url = settings.CELERY_BROKER_URL
app.conf.result_backend = settings.CELERY_RESULT_BACKEND

# Task autodiscovery - empty for now
# app.autodiscover_tasks(['nexus.jobs'])
```

**Export for celery CLI:**

```python
# apps/worker/__init__.py
from .main import app as celery_app
__all__ = ['celery_app']
```

**Important:**

- worker must start cleanly with `celery -A apps.worker.main worker`
- no tasks are scheduled or run in normal flow

---

## 8) Makefile Extensions

Add targets (composable, no duplication):

```makefile
# Infrastructure
infra-up:
	docker compose -f docker/docker-compose.yml up -d

infra-down:
	docker compose -f docker/docker-compose.yml down

infra-logs:
	docker compose -f docker/docker-compose.yml logs -f

# Development
worker:
	celery -A apps.worker.main worker --loglevel=info

dev-api:
	uvicorn apps.api.main:app --reload --port 8000

# Database
migrate:
	alembic -c migrations/alembic.ini upgrade head

migrate-down:
	alembic -c migrations/alembic.ini downgrade -1

# Testing
test:
	pytest python/tests -v

test-ci:
	pytest python/tests -v --tb=short
```

Notes:

- `make dev` target deferred (web not ready)
- `make infra-up` required before `make dev-api` or `make worker`

---

## 9) CI Workflow

Create/update `.github/workflows/ci.yml`:

```yaml
name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15.8
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: nexus_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7.2-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    env:
      NEXUS_ENV: test
      NEXUS_INTERNAL_SECRET: test-secret-for-ci
      DATABASE_URL: postgresql+psycopg://postgres:postgres@localhost:5432/nexus_test
      REDIS_URL: redis://localhost:6379/0
      CELERY_BROKER_URL: redis://localhost:6379/0
      CELERY_RESULT_BACKEND: redis://localhost:6379/0

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install -e ".[dev]"

      - name: Wait for services
        run: |
          until pg_isready -h localhost -p 5432; do sleep 1; done
          until redis-cli -h localhost ping; do sleep 1; done

      - name: Run migrations
        run: make migrate

      - name: Run tests
        run: make test-ci
```

**Key points:**

- Explicit env vars (do not rely on .env.example at runtime)
- Real postgres:15.8 and redis:7.2-alpine
- Health checks before tests
- `.env.example` is documentation only, not used in CI

---

## 10) Internal Header Enforcement

**Behavior (unchanged from S0):** FastAPI middleware rejects requests without valid `X-Nexus-Internal` header.

**PR-01 responsibility:** Ensure environment variables are documented and CI exports them.

- `.env.example` contains `NEXUS_INTERNAL_SECRET`
- CI workflow exports `NEXUS_INTERNAL_SECRET`
- Middleware behavior already exists in S0

Tests must include the header. Test helpers from S0 (`auth_headers()`) should auto-include it.

---

## 11) Testing Requirements

**Must Pass**

- migrations apply cleanly (`make migrate` on fresh db)
- migrations roll back cleanly (`make migrate-down`)
- unique indexes enforce constraints (test violations)
- celery app initializes (`make worker` starts without error)
- redis reachable (worker can connect)
- existing S0 tests still pass

**Specific tests to add:**

```python
def test_migration_applies_cleanly(fresh_db):
    """Migration 0002 applies to 0001 state."""
    # alembic upgrade head succeeds

def test_migration_rollback(migrated_db):
    """Migration 0002 rolls back cleanly."""
    # alembic downgrade -1 succeeds

def test_canonical_url_uniqueness(db_session):
    """Partial unique index on (kind, canonical_url) enforced."""
    # Insert two media with same kind + canonical_url → IntegrityError

def test_file_sha256_uniqueness_per_user(db_session):
    """Partial unique index on (user, kind, sha256) enforced."""
    # Same user, same sha256 → IntegrityError
    # Different user, same sha256 → OK

def test_redis_connectivity():
    """Celery broker is reachable."""
    from redis import Redis
    r = Redis.from_url(settings.REDIS_URL)
    assert r.ping()

def test_celery_app_initializes():
    """Worker app can be imported without error."""
    from apps.worker.main import app
    assert app.conf.broker_url
```

**Not Required (yet)**

- no ingestion tests
- no retry tests
- no storage tests

---

## 12) Non-Negotiable Constraints

- no extractors
- no fake fragment creation
- no job enqueueing
- no UI behavior changes
- no schema drift beyond this spec
- use guarded DO blocks for enum creation (not IF NOT EXISTS)
- no skipping migrations in CI
- processing_attempts must be NOT NULL
- size_bytes must be bigint

---

## 13) Definition of Done

PR-01 is complete when:

- `make infra-up` starts postgres + redis
- `make migrate` works on a fresh db
- `make migrate-down` rolls back cleanly
- `make worker` boots without error (connects to redis)
- CI passes green
- schema matches S1 spec exactly
- enum types created with proper guards
- all NOT NULL constraints enforced
- future PRs can build ingestion logic without schema changes
