# Nexus — L3 PR Spec: S1-PR-04

## Storage + Upload + Ingest + File Idempotency + Upload UI

This PR makes file-backed media (PDF, EPUB) real end-to-end, without scheduling extraction jobs.

It establishes the storage boundary, secure direct uploads, deterministic deduplication, and signed download access, while preserving all S1 invariants.

---

## 0) Dependencies

**Requires S1-PR-01 merged** with:
- `media_file` table created
- `media` table S1 columns (`processing_status`, `failure_stage`, `file_sha256`, etc.)
- `ProcessingStatus` and `FailureStage` enums
- Partial unique index on `(created_by_user_id, kind, file_sha256)`

Do not attempt PR-04 before PR-01 migration is applied.

---

## 0.1) Scope Summary

### This PR DOES

- Create Supabase Storage client abstraction
- Enable direct-to-storage uploads (signed PUT)
- Implement ingest confirm with synchronous hashing + dedupe
- Enforce permissions + visibility on all storage access
- Add signed download URLs
- Add minimal upload UI + status feedback
- Add storage-backed integration tests

### This PR DOES NOT

- Enqueue Celery tasks
- Create fragments
- Transition media beyond pending
- Implement any extractors
- Touch transcripts, highlights, or search

---

## 1) Storage Architecture

### Bucket

- **Bucket name:** `media`
- **Visibility:** private (no public access)
- **Access:** signed URLs only

### Path Invariant

Canonical stored path is **always relative** (no leading slash):

| Environment | `storage_path` in DB |
|-------------|----------------------|
| Production | `media/{media_id}/original.{ext}` |
| Test | `test_runs/{run_id}/media/{media_id}/original.{ext}` |

**Rules:**
- `storage_path` stored in `media_file` is the **full relative path** including any prefix
- No leading slash: `media/abc/original.pdf` (correct), NOT `/media/abc/original.pdf`
- No user identifiers in paths
- `StorageClient` methods receive the full `storage_path` directly — no prefix manipulation at call site

---

## 2) Storage Client Abstraction

**File:** `python/nexus/storage/client.py`

```python
@dataclass
class SignedUpload:
    """Supabase signed upload response."""
    path: str
    token: str
    # Use supabase.storage.uploadToSignedUrl(path, token, file) on client

@dataclass
class ObjectMetadata:
    """Advisory only — do not trust for validation."""
    content_type: str
    size_bytes: int

class StorageClient:
    def sign_upload(
        self,
        path: str,
        *,
        content_type: str,
        expires_in: int = 300,
    ) -> SignedUpload
    # Returns { path, token } for use with uploadToSignedUrl()

    def sign_download(
        self,
        path: str,
        *,
        expires_in: int = 300,
    ) -> str
    # Returns signed download URL

    def head_object(self, path: str) -> ObjectMetadata | None
    # Returns None if object doesn't exist, else metadata.
    # ONLY trusted use: existence check (None vs not-None).
    # Size/type fields are advisory — do NOT trust for validation.

    def stream_object(self, path: str) -> Iterator[bytes]
    # Used for hashing + validation from actual bytes

    def delete_object(self, path: str) -> None
```

### Path Building (Single Point of Prefix Logic)

**File:** `python/nexus/storage/paths.py`

```python
def build_storage_path(media_id: UUID, ext: str) -> str:
    """
    Build the full storage path for a media file.
    Called ONLY by upload/init and test fixtures.

    Returns:
        - Production: "media/{media_id}/original.{ext}"
        - Test: "test_runs/{run_id}/media/{media_id}/original.{ext}"
    """
    prefix = _get_test_prefix()  # "" in prod, "test_runs/{run_id}/" in test
    return f"{prefix}media/{media_id}/original.{ext}"

def _get_test_prefix() -> str:
    # Internal: reads from env/fixture, returns "" or "test_runs/{run_id}/"
    # NEVER call this outside build_storage_path
```

**Critical rule:** `StorageClient` methods receive the **full `storage_path`** verbatim. No prefix logic exists at StorageClient call sites. Prefix is applied exactly once in `build_storage_path()`.

**Why:** Prevents double-prefix bugs like `test_runs/.../test_runs/...` which would cause prod to delete wrong paths or sign wrong URLs.

---

## 3) API Endpoints

### 3.1 POST /media/upload/init

**Purpose:** Create media stub + mint signed upload URL.

| Property | Value |
|----------|-------|
| Auth | authenticated user |
| Visibility | creates readable media by adding to default library |

#### Request

```json
{
  "kind": "pdf" | "epub",
  "filename": "document.pdf",
  "content_type": "application/pdf",
  "size_bytes": 1048576
}
```

#### Validation

- `kind` ∈ {pdf, epub}
- `content_type` matches kind
- `size_bytes` ≤ hard cap:
  - PDF: 100 MB
  - EPUB: 50 MB

#### Behavior

**Ordering is critical:** Mint the signed URL before persisting rows to avoid orphan rows on signing failure.

1. Generate `media_id` (UUID) — do NOT persist yet
2. Compute `storage_path` via `build_storage_path(media_id, ext)`
3. **Attempt to mint signed upload URL** via `StorageClient.sign_upload(storage_path, ...)`
   - If signing fails:
     - **Log:** `request_id`, proposed `media_id`, `storage_path`, error details
     - **Return:** `500 E_SIGN_UPLOAD_FAILED`
     - **No rows created** (nothing to clean up)
4. **Only after signing succeeds**, persist in a single transaction:
   - Create media row (`processing_status = pending`, `created_by_user_id = viewer.user_id`)
   - Create media_file row with `storage_path`
   - Insert library_media into viewer's default library
5. Return upload info

**Rationale:** Signing is an external call that can fail. If we create DB rows first and signing fails, we leave orphan rows. By signing first, we guarantee no orphan rows on failure.

#### Response

Supabase signed uploads return `{ path, token }`. The browser **must** use `supabase.storage.uploadToSignedUrl(path, token, file)`.

```json
{
  "data": {
    "media_id": "uuid",
    "storage_path": "media/{media_id}/original.pdf",
    "token": "...",
    "expires_at": "2026-01-01T00:05:00Z"
  }
}
```

**Browser upload:** Use `supabase.storage.uploadToSignedUrl(storage_path, token, file)`. This uses the official Supabase client method, avoiding CORS surprises from raw fetch PUT.

#### CORS Contingency Plan

If browser-direct PUT to Supabase Storage is blocked by CORS in real browsers:

**Acceptable pivot (within PR-04 scope):**
1. Browser uploads to Next.js BFF route (`POST /api/media/upload`)
2. Next.js streams to FastAPI or directly to Supabase Storage
3. Ingest confirm still computes SHA from storage (unchanged)
4. Dedupe logic unchanged

**What changes:**
- Upload init returns `{ media_id, upload_endpoint: "/api/media/{id}/upload" }` instead of signed URL
- BFF route handles multipart upload and streams to storage
- Slightly higher latency (extra hop) but no architectural change

**What stays the same:**
- SHA computed at ingest confirm from stored object
- Dedupe via unique constraint
- All failure semantics
- Storage path invariants

This contingency is pre-approved for PR-04 if CORS issues are discovered during implementation.

---

### 3.2 POST /media/{media_id}/ingest

**Purpose:** Confirm upload, hash file, dedupe.

| Property | Value |
|----------|-------|
| Auth | creator only |
| Tasks | No tasks enqueued |

#### Steps

1. Verify caller is `created_by_user_id`
2. **Existence check:** Call `head_object(storage_path)`
   - If `None` → `400 E_STORAGE_MISSING`, mark media `failed`
   - If not `None` → object exists; proceed to streaming validation
   - **Note:** `head_object` metadata (size, type) is advisory only — the ONLY trusted signal is existence (None vs not-None)
3. Stream object and validate **from actual bytes**:
   - Enforce size cap from bytes streamed (not from metadata)
   - Sniff magic bytes from first chunk:
     - PDF: first 5 bytes are `%PDF-`
     - EPUB: first 4 bytes are ZIP header (`PK\x03\x04`)
   - If size exceeds cap → `400 E_FILE_TOO_LARGE`, mark media `failed`
   - If magic bytes invalid → `400 E_INVALID_FILE_TYPE`, mark media `failed`
   - Compute SHA-256 from full stream

**EPUB validation scope (PR-04):** Only validate ZIP header magic bytes. Deeper structural validation (mimetype entry, OPF parsing, etc.) is deferred to S5 extractor. Checking for `mimetype` entry inside the ZIP requires partial archive parsing, which is scope creep for ingest and a frequent source of "works on some EPUBs" bugs.

**Security:** Treat `Content-Type` from storage metadata or client headers as untrusted. Attackers can upload arbitrary bytes with forged content-type. Only magic byte sniffing and size enforcement from actual streamed bytes are authoritative.

#### Streaming Limits (Required)

| Constant | Value | Purpose |
|----------|-------|---------|
| `INGEST_STREAM_TIMEOUT_S` | 60 | Max duration for streaming + hashing |
| `HASH_CHUNK_BYTES` | 8 MiB | Read chunk size for streaming |
| `MAX_PDF_BYTES` | 100 MiB | PDF size cap |
| `MAX_EPUB_BYTES` | 50 MiB | EPUB size cap |

**Timeout behavior:**
- If streaming exceeds `INGEST_STREAM_TIMEOUT_S` → `504 E_INGEST_TIMEOUT`
- Mark media `failed` with `failure_stage=upload`
- User can retry after transient storage slowness resolves

**Rationale:** Without timeouts, a slow storage read can tie up web workers indefinitely.

4. Deduplication (transactional, race-safe):

```python
with transaction():
    # Lock current media row FOR UPDATE to prevent concurrent ingest
    media = select(Media).where(Media.id == media_id).with_for_update()

    # Idempotency: if file_sha256 already set, this ingest already completed
    if media.file_sha256 is not None:
        return media.id, duplicate=False  # Already ingested

    # Capture path BEFORE any deletion (we need it for storage cleanup)
    loser_path = media.storage_path

    existing = find_media_by_hash(
        user_id=media.created_by_user_id,
        kind=media.kind,
        sha256=computed_sha,
    )

    if existing and existing.id != media.id:
        # DUPLICATE: delete loser row, return winner
        delete(media)                    # DB cascade deletes media_file
        ensure_in_default_library(existing.id)
        commit()

        storage.delete_object(loser_path)  # best-effort, uses captured path
        return existing.id, duplicate=True

    # Not a duplicate: set sha256
    # NOTE: If concurrent ingest races past the pre-check, the unique
    # constraint on (created_by_user_id, kind, file_sha256) will fire.
    try:
        media.file_sha256 = computed_sha
        flush()  # Force constraint check
    except UniqueViolation:
        # Race condition: another ingest won. Look up winner and dedupe.
        rollback()
        winner = find_media_by_hash(
            user_id=media.created_by_user_id,
            kind=media.kind,
            sha256=computed_sha,
        )
        with transaction():
            delete(media)
            ensure_in_default_library(winner.id)
        storage.delete_object(loser_path)  # best-effort
        return winner.id, duplicate=True
```

**Race handling:** The unique constraint `(created_by_user_id, kind, file_sha256)` is the ultimate arbiter. If `find_media_by_hash` misses a concurrent insert, the constraint fires, and we catch `UniqueViolation` to resolve to the winner.

5. Leave `processing_status = pending` (no task enqueued in PR-04)

#### Response

```json
{
  "data": {
    "media_id": "uuid",
    "duplicate": false
  }
}
```

---

### 3.3 GET /media/{media_id}/file

**Purpose:** Signed download access.

| Property | Value |
|----------|-------|
| Auth | `can_read_media(viewer, media)` must pass |

#### Behavior

- 404 if unreadable
- 404 if no media_file
- Mint signed GET URL (5 min)

#### Response

```json
{
  "data": {
    "url": "https://...",
    "expires_at": "2026-01-01T00:05:00Z"
  }
}
```

---

## 4) Permissions Model (Non-Negotiable)

| Endpoint | Requirement |
|----------|-------------|
| `upload/init` | authenticated |
| `ingest` | creator only |
| `download` | `can_read_media` |

Admins do not get ingest rights.

---

## 5) Deduplication Guarantees

- **Scope:** per user, per kind
- **Key:** `(created_by_user_id, kind, file_sha256)`
- Enforced by DB unique index
- Race-safe: relies on constraint + retry logic
- Loser row is deleted; storage cleaned up best-effort

---

## 6) Failure Semantics

| Failure | HTTP Response | Media State |
|---------|---------------|-------------|
| Upload never confirmed | N/A | `pending` (see §6.1 cleanup) |
| Object missing at ingest | `400 E_STORAGE_MISSING` | `failed`, `failure_stage=upload` |
| Hash exceeds size cap | `400 E_FILE_TOO_LARGE` | `failed`, `failure_stage=upload` |
| Magic bytes invalid | `400 E_INVALID_FILE_TYPE` | `failed`, `failure_stage=upload` |
| Signing failure at init | `500 E_SIGN_UPLOAD_FAILED` | Row rolled back (no orphan) |
| Duplicate detected | `200` with winner | Loser deleted |
| Storage delete fails | N/A | log + janitor cleanup |

**Failed transitions in PR-04:** Invalid uploads (bad file type, too large, missing object) transition to `failed` with `failure_stage=upload`. This prevents zombie `pending` rows that can never succeed. Users can call `POST /media/{id}/retry` (S1-PR-05) to retry after fixing the issue.

### 6.2) Failure vs Dedupe: Critical Distinction

| Scenario | Action | Row State |
|----------|--------|-----------|
| Invalid upload (bad type, too large, missing) | **Mark `failed`** | Row persists, user can retry |
| Duplicate detected | **Delete loser row** | Row removed, winner returned |

**Hard rules:**
- Invalid upload errors must **NOT** delete the media row — mark it `failed` instead
- Dedupe must **delete** the loser row — do NOT mark it `failed`
- `failure_stage` is set **only** for non-dedupe failures

**Rationale:** If we delete on validation failure, users lose the ability to retry. If we mark dedupe losers as `failed`, we leave zombie rows that can never succeed.

### 6.1) Abandoned Upload Cleanup Policy

Media in `pending` with `media_file` created but `file_sha256` still null (upload never confirmed) is considered **abandoned** after a configurable threshold (default: 24 hours).

**Cleanup rules:**
- A scheduled job (or manual admin command) identifies abandoned media
- Deletes storage objects for abandoned uploads
- Deletes the orphan `media` + `media_file` rows
- For v1: implement as a manual admin CLI command; automated janitor is v2

**Rationale:** Without cleanup, failed/abandoned uploads leak storage objects indefinitely.

---

## 7) UI Changes (Minimal)

### Upload Flow

- File picker (PDF/EPUB)
- Progress during PUT
- "Queued (extractor not available yet)" badge after ingest
- Duplicate → redirect to existing media

### Components

- `ProcessingStatusBadge`
- Upload modal / inline panel

No extraction indicators.

---

## 8) Tests (Required)

### Required Test Classes

#### Storage (marked `@pytest.mark.storage`)

- Signed upload returns `{ path, token }` for `uploadToSignedUrl()`
- Signed download URL requires visibility
- Non-member gets 404
- Test prefix isolation (`test_runs/{run_id}`)
- **Upload/init signing failure** → `500 E_SIGN_UPLOAD_FAILED`, no orphan rows
- **GET /media/{id}** includes `capabilities.can_download_file` based on `media_file` existence

#### Ingest

- Hash computed correctly from streamed bytes (not metadata)
- Same file + same user → dedupe, loser row deleted, storage cleaned
- Same file + different user → no dedupe (separate rows)
- Magic byte validation (PDF: `%PDF-`, EPUB: ZIP + mimetype)
- Size enforcement from actual bytes (not metadata)
- **Concurrent ingest race** → unique constraint resolves to single winner, no orphans
  - **Test implementation:** Must use **two independent DB sessions** (two separate connections), each calling ingest concurrently, to actually trigger `UniqueViolation`. A single shared connection with nested transactions will pass trivially and not cover the real race.
- **Invalid file type** → `400 E_INVALID_FILE_TYPE`, media marked `failed`
- **File too large** → `400 E_FILE_TOO_LARGE`, media marked `failed`
- **Object missing** → `400 E_STORAGE_MISSING`, media marked `failed`

#### Permissions

- Non-creator cannot ingest
- Member cannot download unless readable

---

## 9) GET /media/{id} Updates (PR-04 Requirement)

PR-04 must update the existing `GET /media/{id}` endpoint to:

1. **Join `media_file` existence** to determine `media_file_exists`
2. **Compute capabilities** using `derive_capabilities()` with `media_file_exists` as input
3. **Include in response:** `capabilities.can_download_file` (true iff `media_file` exists and viewer can read)

For `GET /media` (list endpoint):
- Use bulk `media_file` existence check for efficiency
- Include `capabilities` on each item

**Capabilities scope (PR-04):** Only `can_download_file` is newly influenced by `media_file` existence. All other capabilities (`can_read`, `can_quote`, `can_highlight`, `can_search`, `can_play`) remain as defined in PR-03 — do NOT change their semantics here. In particular:
- `can_quote` for PDF depends on `plain_text` existence (PR-03 definition), unchanged by PR-04
- `can_read` for PDF depends on `media_file` existence (PR-03 definition), unchanged by PR-04

**Rationale:** UI relies on `capabilities.can_download_file` to show/hide download buttons. Without joining `media_file`, capabilities will be wrong and violate the L2 contract.

---

## 10) Non-Negotiable Invariants (Re-stated)

- Server never proxies file bytes (except streaming for hash)
- Storage URLs are always signed + short-lived
- Dedupe resolved by DB constraint, not pre-check alone
- **Storage side effects MUST occur after transaction commit:**
  ```python
  # CORRECT:
  with transaction():
      delete(media)
      commit()
  storage.delete_object(loser_path)  # After commit, in try/except

  # WRONG:
  with transaction():
      delete(media)
      storage.delete_object(loser_path)  # NEVER inside transaction
      commit()
  ```
  Storage operations after commit must be wrapped in try/except that logs but never changes the HTTP response.
- Invalid uploads transition to `failed` (not stuck `pending`)
- Dedupe losers are deleted (not marked `failed`)
- UI uses capabilities, not status guesses

---

## 11) Exit Criteria

PR-04 is complete when:

- [ ] A user can upload a PDF
- [ ] The file is stored privately
- [ ] The hash is computed
- [ ] Duplicate uploads collapse correctly (including race conditions)
- [ ] Invalid uploads (bad type, too large, missing) transition to `failed`
- [ ] The file can be downloaded only by authorized viewers
- [ ] `GET /media/{id}` includes correct `capabilities.can_download_file`
- [ ] Storage tests pass in CI (see §11.1)
- [ ] No Celery task is enqueued

### 11.1) CI Storage Test Policy

| Context | Behavior |
|---------|----------|
| Protected branches (main) with secrets | Storage tests **required**, must pass |
| PRs from forks (no secrets) | Storage tests **skipped** (not failed) |
| Local dev without secrets | Storage tests **skipped** |

**Implementation:**
- Storage tests marked with `@pytest.mark.storage`
- CI conditionally runs: `if: ${{ secrets.SUPABASE_URL != '' }}`
- Tests use `pytest.importorskip` or `skipif` when secrets missing

**Non-storage tests:**
- A **fake storage implementation** (`FakeStorageClient`) must exist for unit tests
- Unit tests for `sign_upload`, `sign_download`, `object_exists` logic run without real storage
- Integration tests with real Supabase run only when secrets available

**Rationale:** Contributors on forks can't access secrets, so storage tests must skip gracefully. Core logic is still tested via fake implementation.
